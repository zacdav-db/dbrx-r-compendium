{
  "hash": "09d638e0f3306c0ca9cbdd88175ed7cc",
  "result": {
    "markdown": "# Persisting Packages\n\n::: callout-important\nEvaluate if [faster package installs](https://zacdav-db.github.io/dbrx-r-compendium/chapters/pkg-management/fast-installs.html) is able to solve any installation pain-points before investigating persisting packages, faster installs is easier to set-up and manage.\n:::\n\nDatabricks clusters are ephemeral and therefore any installed packages will not be available on restart. If the cluster has cluster libraries defined then those libraries are installed after the cluster is started - this can be time consuming when there are multiple packages.\n\nThe article on [faster package installs](https://zacdav-db.github.io/dbrx-r-compendium/chapters/pkg-management/fast-installs.html) details how to reduce the time it takes to install each package. Faster installs are great, but sometimes it's preferable to not install at all and persist the packages required, similar to how you'd use R locally.\n\n## Where Packages are Installed\n\nWhen installing packages with `install.packages` the default behaviour is that they'll be installed to the first element of `.libPaths()`.\n\n`.libPaths()` returns the paths of \"R library trees\", directories that R packages can reside. When you load a package it will be loaded from the first location it is found as dictated by `.libPaths()`.\n\nWhen working within a Databricks notebook `.libPaths()` will return 6 values by default, in order they are:\n\n| Path                                                 | Details                                                                                                                                                                                                             |\n|-----------------------------------------------------|-------------------|\n| `/local_disk0/.ephemeral_nfs/envs/rEnv-<session-id>` | The first location is always a notebook specific directory, this is what allows [each notebook session to have different libraries installed](https://docs.databricks.com/en/libraries/notebooks-r-libraries.html). |\n| `/databricks/spark/R/lib`                            | Only `{SparkR}` is found here                                                                                                                                                                                       |\n| `/local_disk0/.ephemeral_nfs/cluster_libraries/r`    | [Cluster libraries](https://docs.databricks.com/en/libraries/cluster-libraries.html) - you could also install packages here explicitly to share amongst all users (e.g. `lib` parameter of `install.packages`)      |\n| `/usr/local/lib/R/site-library`                      | Packages built into the [Databricks Runtime](https://www.databricks.com/glossary/what-is-databricks-runtime)                                                                                                        |\n| `/usr/lib/R/site-library`                            | Empty                                                                                                                                                                                                               |\n| `/usr/lib/R/library`                                 | Base R packages                                                                                                                                                                                                     |\n\nIt's important to understand that the order defines the default behaviour as It's possible to add or remove values in `.libPaths()`. You'll almost certainly be adding values, there's little reason to remove values.\n\n::: callout-note\nAll following examples will use [Unity Catalog Volumes](https://docs.databricks.com/en/connect/unity-catalog/volumes.html). [DBFS](https://docs.databricks.com/en/dbfs/index.html) can be used but it's not recommended.\n:::\n\n## Persisting a Package\n\nThe recommended approach is to first install the library(s) you want to persist on a cluster via a notebook.\n\nFor example, let's persist [`{leaflet}`](https://rstudio.github.io/leaflet/) to a volume:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"leaflet\") #<1>\n\n# determine where the package was installed\npkg_location <- find.package(\"leaflet\") #<2>\n\n# move package to volume\nnew_pkg_location <- \"/Volumes/<catalog>/<schema>/<volume>/my_packages\"  #<3>\nfile.copy(from = pkg_location, to = new_pkg_location, recursive = TRUE)  #<4>\n```\n:::\n\n\n1.  Installing `{leaflet}`\n\n2.  Return path to package files, from what was explained before we know this will be a sub-directory of `.libPaths()` first path\n\n3.  Define the path to volume where package will be persisted, make sure to adjust as needed\n\n4.  Copy the folder contents recursively to the volume\n\nAt this point the package is persisted, but if you restart the cluster or detach and reattach and try to load `{leaflet}` it will fail to load.\n\nThe last step is to adjust `.libPaths()` to include the volume path. You could make it the first value by:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# adjust .libPaths\n.libPaths(c(new_pkg_location, .libPaths())) #<1>\n```\n:::\n\n\n1.  I recommend against making it the first value, will detail why in [Ordering]\n\n## Adjusting `.libPaths()`\n\n### Ordering\n\nGiven that `.libPaths()` can return 6 values in a notebook you might wonder if there a \"best\" position to add your new volume path(s) to, that will depend on how you want packages to behave.\n\nA safe default is to add a path *after* the cluster libraries location (currently 3rd), this will make it appear as if the Databricks Runtime has been extended to include packages in the volume path(s).\n\nAlternatively you could add it after the first path and all users will still have the notebook scope package behaviour by default but cluster libraries may not load if they appear in the earlier paths under a different version.\n\nIt will be up to you to decide what works best.\n\n::: callout-important\nI don't recommend pre-pending `.libPaths()` with volume paths as packages will attempt to install to the first value and you cannot directly install packages to a volume path (due to volumes being backed onto cloud storage). This is why the example for persisting copies after installation.\n:::\n\nAn example of adjusting `.libPaths()` looks like:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvolume_pkgs <- \"/Volumes/<catalog>/<schema>/<volume>/my_packages\"\n.libPaths(new = append(.libPaths(), volume_pkgs, after = 3))\n```\n:::\n\n\n### Helpful Functions\n\nThe examples can be used to build a set of functions to make this easier.\n\n**Copying a Package**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncopy_package <- function(name, destination) {\n  package_loc <- find.package(name)\n  file.copy(from = package_loc, to = destination, recursive = TRUE)\n}\n\n# e.g. move {ggplot2} to volume\ncopy_package(\"ggplot2\", \"/Volumes/<catalog>/<schema>/<volume>/my_packages\")\n```\n:::\n\n\n**Alter `.libPaths()`**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nadd_lib_paths <- function(path, after, version = FALSE) {\n  if (version) { #<1>\n    rver <- getRversion() #<1>\n    lib_path <- file.path(path, rver) #<1>\n  } else { #<1>\n    lib_path <- file.path(path) #<1>\n  } #<1>\n\n  # ensure directory exists\n  if (!file.exists(lib_path)) {\n    dir.create(lib_path, recursive = TRUE)\n  }\n\n  lib_path <- normalizePath(lib_path, \"/\")\n\n  message(\"primary package path is now \", lib_path)\n  .libPaths(new = append(.libPaths(), lib_path, after = after))\n  lib_path\n}\n```\n:::\n\n\n1.  Allows specifying `version` as `TRUE` or `FALSE` to suffix the supplied `path` with the current R version\n\n### Avoiding Repetition\n\nTo avoid manually adjusting `.libPaths()` every notebook you can craft an [init script](https://docs.databricks.com/en/init-scripts/index.html) or set [environment variables](https://docs.databricks.com/en/compute/configure.html#environment-variables), depending on the desired outcome.\n\n::: callout-caution\nIn practice this interferes with how Databricks sets up the environment, validate any changes thoroughly before rolling out to users.\n:::\n\n#### Init Script\n\n::: callout-note\nThis example appends to the existing `Renviron.site` file to ensure any settings defined as part of runtime are preserved.\n\nThe last two lines of the script are setting `R_LIBS_SITE` and `R_LIBS_USER`. Changing these lines can give you granular control over order for anything after the 1st value of `.libPaths()` as it's injected when the notebook session starts.\n:::\n\n\n::: {.cell}\n\n```{.bash .cell-code}\n#!/bin/bash\nvolume_pkgs=/Volumes/<catalog>/<schema>/<volume>/my_packages #<1>\ncat <<EOF >> \"/etc/R/Renviron.site\"\nR_LIBS_USER=%U:/databricks/spark/R/lib:/local_disk0/.ephemeral_nfs/cluster_libraries/r:$volume_pkgs # <2>\nEOF\n```\n:::\n\n\n1.  Define the path(s) to add to `R_LIBS_USER`\n\n2.  Append line to `/etc/R/Renviron.site` with location after cluster libraries, you can rearrange the paths as long as they remain `:` separated\n\n#### Environment Variables\n\n::: callout-caution\nHow the Databricks Runtime defines and uses the R environment variables is something that may change and should be tested carefully, especially if upgrading runtime versions.\n:::\n\nThere are particular environment variables (`R_LIBS`, `R_LIBS_USER`, `R_LIBS_SITE`) that can be set to initialise the library search path (`.libPaths()`).\n\n`R_LIBS` and `R_LIBS_USER` are defined as part of start-up processes in Databricks Runtime and they'll be overridden, it's easier to adjust via an [Init Script].\n\n`R_LIBS_SITE` can be set via an [environment variable](https://docs.databricks.com/en/compute/configure.html#environment-variables) but is referenced by `/etc/R/Renviron.site` and will provides limited control over where the path will appear in the `.libPaths()` order (it will appear 5th, after the packages included in the Databricks runtime) unless using an init script to alter `/etc/R/Renviron.site` directly.\n\n## Organising Packages\n\nWhen going down this route of persisting packages you should consider how this is organised and managed long term to avoid making things messy.\n\nSome practices you can consider include:\n\n-   Maintaining directories of packages per project, team, or user\n\n-   Ensuring directories are specific to an R version (and potentially even Databricks Runtime version)\n\n-   Coupling the use of persistence with [`{renv}`](https://rstudio.github.io/renv/articles/renv.html)\n",
    "supporting": [
      "persisting-libs_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}