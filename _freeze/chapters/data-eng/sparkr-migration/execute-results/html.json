{
  "hash": "4bb4290a789126b02b942bc9bdbd53cf",
  "result": {
    "engine": "knitr",
    "markdown": "# `{SparkR}` to `{sparklyr}`\n\n## Introduction\n\nBeginning with Spark `4.x`, [`{SparkR}` will be deprecated](https://spark.apache.org/docs/4.0.0-preview2/sparkr-migration-guide.html#upgrading-from-sparkr-35-to-40). Going forward, `{sparklyr}` will be the recommended R package for working with Apache Spark. This guide is intended to help users understand the differences between `{SparkR}` and `{sparklyr}` across Spark APIs, and aid in code migration from one to the other. It combines basic concepts with specific function mappings where appropriate.\n\n### Overview of `{SparkR}` and `{sparklyr}`\n\n`{SparkR}` and `{sparklyr}` are both R packages designed to work with Apache Spark, but differ significantly in design, syntax, and integration with the broader R ecosystem.\n\n`{SparkR}` is developed as part of Apache Spark itself, and its design mirrors Spark’s core APIs. This makes it straightforward for those familiar with Spark’s other language interfaces - Scala and Python. However, this may be less intuitive for R users accustomed to the [tidyverse](https://www.tidyverse.org/).\n\nIn contrast, `{sparklyr}` is developed and maintained by [Posit PBC](https://posit.co/) with a focus on providing a more R-friendly experience. It leverages `{dplyr}` syntax, which is highly familiar to users of the `{tidyverse}`, enabling them to interact with Spark DataFrames using R-native verbs like `select()`, `filter()`, and `mutate()`. This makes `{sparklyr}` easier to learn for R users, especially those who are not familiar with Spark’s native API.\n\n## Environment setup\n\n### Installation\n\nIf working inside of the Databricks Workspace, no installation is required - you can simply load `{sparklyr}` with `library(sparklyr)`. To install `{sparklyr}` on a machine outside of Databricks, [follow these steps](https://spark.posit.co/get-started/).\n\n### Connecting to Spark\n\nWhen working inside of the Databricks workspace, you can connect to Spark with `{sparklyr}` with the following code:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sparklyr)\nsc <- spark_connect(method = \"databricks\")\n```\n:::\n\n\n\n\n\n\n\n\nWhen connecting to Databricks remotely via [Databricks Connect](https://docs.databricks.com/en/dev-tools/databricks-connect/index.html), a slightly different method is used:\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsc <- spark_connect(method = \"databricks_connect\")\n```\n:::\n\n\n\n\n\n\n\n\nFor more details and an extended tutorial on Databricks Connect with `{sparklyr}`, see the [official documentation](https://spark.posit.co/deployment/databricks-connect.html).\n\n## Reading & Writing Data\n\nIn contrast to generic `read.df()` and `write.df()` [functions in `{SparkR}`](https://spark.apache.org/docs/3.5.2/api/R/articles/sparkr-vignettes.html#data-sources), `{sparklyr}` has a family of `spark_read_*()`and `spark_write_*()` [functions](https://spark.posit.co/packages/sparklyr/latest/reference/#spark-data) to load and save data. There are also unique functions to create Spark DataFrames or Spark SQL [temporary views](https://spark.apache.org/docs/latest/sql-ref-syntax-ddl-create-view.html) from R data frames in memory.\n\n### TL;DR\n\n| `{SparkR}` | `{sparklyr}` |\n|----|----|\n| `createDataFrame()` | `copy_to()` |\n| `createOrReplaceTempView()` | Use `invoke()` with method directly |\n| `saveAsTable()` | `spark_write_table()` |\n| `write.df()` | `spark_write_<format>()` |\n| `tableToDF()` | `tbl()` (or `spark_read_table()` when it's fixed) |\n| `read.df()` | `spark_read_<format>()` |\n\n: Recommended function mapping\n\n### Loading Data\n\nTo convert a R data frame to a Spark DataFrame, or to create a temporary view out of a DataFrame to apply SQL to it:\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create SparkDataFrame from R data frame\nmtcars_df <- createDataFrame(mtcars)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create SparkDataFrame and name temporary view 'mtcars_tmp'\nmtcars_tbl <- copy_to(\n  sc,\n  df = mtcars,\n  name = \"mtcars_tmp\", # <1>\n  overwrite = TRUE,\n  memory = FALSE # <2>\n) \n```\n:::\n\n\n\n\n\n\n\n\n1.  `copy_to()` will create a temporary view of the data with the given `name`, you can use name to reference data if using SQL directly (e.g. `sdf_sql()`).\n2.  Default behaviour of `copy_to()` will set `memory` as `TRUE` which caches the table. This helps when reading the data multiple times - sometimes its worth setting to `FALSE` if data is read as one-off.\n\n### Creating Views\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create temporary view\ncreateOrReplaceTempView(mtcars_df, \"mtcars_tmp_view\")\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# direct equivlent from SparkR requires `invoke`\n# usually redundant given `copy_to` already creates a temp view\nspark_dataframe(mtcars_tbl) |>\n  invoke(\"createOrReplaceTempView\", \"mtcars_tmp_view\")\n```\n:::\n\n\n\n\n\n\n\n\n### Writing Data\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save SparkDataFrame to Unity Catalog\nsaveAsTable(\n  mtcars_df,\n  tableName = \"<catalog>.<schema>.<table>\",\n  mode = \"overwrite\"\n)\n\n# save DataFrame using delta format to local filesystem\nwrite.df(\n  mtcars_df,\n  path = \"file:/<path/to/save/delta/mtcars>\",\n  source = \"delta\", # <1>\n  mode = \"overwrite\"\n)\n```\n:::\n\n\n\n\n\n\n\n\n1.  `write.df()` supports other formats via `source` parameter\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# save tbl_spark to Unity Catalog\nspark_write_table(\n  mtcars_tbl,\n  name = \"<catalog>.<schema>.<table>\",\n  mode = \"overwrite\"\n)\n\n# save tbl_spark using delta format to local filesystem\nspark_write_delta(\n  mtcars_tbl,\n  path = \"file:/<path/to/save/delta/mtcars>\",\n  mode = \"overwrite\"\n)\n\n# Using {DBI}\nlibrary(DBI)\ndbWriteTable(\n  sc,\n  value = mtcars_tbl,\n  name = \"<catalog>.<schema>.<table>\",\n  overwrite = TRUE\n)\n```\n:::\n\n\n\n\n\n\n\n\n### Reading Data\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load Unity Catalog table as SparkDataFrame\ntableToDF(\"<catalog>.<schema>.<table>\")\n\n# load csv file into SparkDataFrame\nread.df(\n  path = \"file:/<path/to/read/csv/data.csv>\",\n  source = \"csv\",\n  header = TRUE,\n  inferSchema = TRUE\n)\n\n# load delta from local filesystem as SparkDataFrame\nread.df(\n  path = \"file:/<path/to/read/delta/mtcars>\",\n  source = \"delta\"\n)\n\n# load data from a table using SQL\n# recommended to use `tableToDF`\nsql(\"SELECT * FROM <catalog>.<schema>.<table>\")\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# currently has an issue if using Unity Catalog\n# recommend using `tbl` (example below)\nspark_read_table(sc, \"<catalog>.<schema>.<table>\", memory = FALSE)\n\n# load table from Unity Catalog with {dplyr}\ntbl(sc, \"<catalog>.<schema>.<table>\")\n\n# or using `in_catalog`\ntbl(sc, in_catalog(\"<catalog>\", \"<schema>\", \"<table>\"))\n\n# load csv from local filesystem as tbl_spark\nspark_read_csv(\n  sc,\n  name = \"mtcars_csv\",\n  path = \"file:/<path/to/delta/mtcars>\",\n  header = TRUE,\n  infer_schema = TRUE\n)\n\n# load delta from local filesystem as tbl_spark\nspark_read_delta(\n  sc,\n  name = \"mtcars_delta\",\n  path = \"file:/tmp/test/sparklyr1\"\n)\n\n# using SQL\nsdf_sql(sc, \"SELECT * FROM <catalog>.<schema>.<table>\")\n```\n:::\n\n\n\n\n\n\n\n\n## Processing Data\n\n### Select, Filter\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select specific columns\nselect(mtcars_df, \"mpg\", \"cyl\", \"hp\")\n\n# filter rows where mpg > 20\nfilter(mtcars_df, mtcars_df$mpg > 20)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select specific columns\nmtcars_tbl |>\n  select(mpg, cyl, hp)\n\n# filter rows where mpg > 20\nmtcars_tbl |>\n  filter(mpg > 20)\n```\n:::\n\n\n\n\n\n\n\n\n### Adding Columns\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add a new column 'power_to_weight' (hp divided by wt)\nwithColumn(mtcars_df, \"power_to_weight\", mtcars_df$hp / mtcars_df$wt)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# add a new column 'power_to_weight' (hp divided by wt)\nmtcars_tbl |>\n  mutate(power_to_weight = hp / wt)\n```\n:::\n\n\n\n\n\n\n\n\n### Grouping & Aggregation\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate average mpg and hp by number of cylinders\nmtcars_df |>\n  groupBy(\"cyl\") |>\n  summarize(\n    avg_mpg = avg(mtcars_df$mpg),\n    avg_hp = avg(mtcars_df$hp)\n  )\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate average mpg and hp by number of cylinders\nmtcars_tbl |>\n  group_by(cyl) |>\n  summarize(\n    avg_mpg = mean(mpg),\n    avg_hp = mean(hp)\n  )\n```\n:::\n\n\n\n\n\n\n\n\n### Joins\n\nSuppose we have another dataset with cylinder labels that we want to join to mtcars.\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create another SparkDataFrame with cylinder labels\ncylinders <- data.frame(\n  cyl = c(4, 6, 8),\n  cyl_label = c(\"Four\", \"Six\", \"Eight\")\n)\ncylinders_df <- createDataFrame(cylinders)\n\n# join mtcars_df with cylinders_df\njoin(\n  x = mtcars_df,\n  y = cylinders_df,\n  mtcars_df$cyl == cylinders_df$cyl,\n  joinType = \"inner\"\n)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create another SparkDataFrame with cylinder labels\ncylinders <- data.frame(\n  cyl = c(4, 6, 8),\n  cyl_label = c(\"Four\", \"Six\", \"Eight\")\n)\ncylinders_tbl <- copy_to(sc, cylinders, \"cylinders\", overwrite = TRUE)\n\n# join mtcars_tbl with cylinders_tbl\nmtcars_tbl |>\n  inner_join(cylinders_tbl, by = join_by(cyl))\n```\n:::\n\n\n\n\n\n\n\n\n## User Defined Functions (UDFs)\n\nSuppose we want to categorize horsepower into ‘High’ or ‘Low’ based on a threshold\n\n::: callout-note\nThis is an arbitrary example; in practice we would recommend [`case_when()`](https://dplyr.tidyverse.org/reference/case_when.html) combined with [`mutate()`](https://spark.posit.co/guides/dplyr.html#dplyr-verbs).\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define custom function\ncategorize_hp <- function(df) {\n  df$hp_category <- ifelse(df$hp > 150, \"High\", \"Low\")\n  df\n}\n```\n:::\n\n\n\n\n\n\n\n\n**SparkR**\n\nUDFs in `{SparkR}` require an output schema, which we define first.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define the schema for the output DataFrame\nschema <- structType(\n  structField(\"mpg\", \"double\"),\n  structField(\"cyl\", \"double\"),\n  structField(\"disp\", \"double\"),\n  structField(\"hp\", \"double\"),\n  structField(\"drat\", \"double\"),\n  structField(\"wt\", \"double\"),\n  structField(\"qsec\", \"double\"),\n  structField(\"vs\", \"double\"),\n  structField(\"am\", \"double\"),\n  structField(\"gear\", \"double\"),\n  structField(\"carb\", \"double\"),\n  structField(\"hp_category\", \"string\")\n)\n```\n:::\n\n\n\n\n\n\n\n\nTo apply this function to each partition of a Spark DataFrame, we use `dapply()`.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# apply function across partitions using dapply\ndapply(\n  mtcars_df,\n  categorize_hp,\n  schema\n)\n```\n:::\n\n\n\n\n\n\n\n\nTo apply the same function to each group of a Spark DataFrame, we use `gapply()`. Note that the schema is still required.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# apply function across groups\ngapply(\n  mtcars_df,\n  cols = \"hp\",\n  func = categorize_hp,\n  schema = schema\n)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n::: callout-tip\nHighly recommended '[Distrubuting R Computations](https://spark.posit.co/guides/distributed-r.html)' guide in `{sparklyr}` docs, it goes into much more detail on `spark_apply()`.\n:::\n\n::: callout-note\n[`spark_apply()`](https://spark.posit.co/packages/sparklyr/latest/reference/spark_apply.html) will do it's best to derive the column names and schema of the output via sampling 10 rows, this can add overhead that can be omitted by specifying the `columns` parameter.\n:::\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# ensure that {arrow} is loaded, otherwise may encounter cryptic errors\nlibrary(arrow)\n\n# apply the function over data \n# by default applies to each partition\nmtcars_tbl |>\n  spark_apply(f = categorize_hp)\n\n# apply the function over data \n# Using `group_by` to apply data over groups\nmtcars_tbl |>\n  spark_apply(\n    f = summary,\n    group_by = \"hp\" # <1>\n  )\n```\n:::\n\n\n\n\n\n\n\n\n1.  In this example `group_by` isn't changing the resulting output as the functions behaviour is applied to rows independently. Other functions that operate on a set of rows would behave differently (e.g. `summary()`).\n\n`SparkR::spark.lapply()` is unique in that it applies to lists in R, as opposed to DataFrames. There is no exact equivalent in `{sparklyr}`, but using `spark_apply()` with a DataFrame with unique IDs and grouping it by ID will behave similarly in many cases, or, more creative functions that operate on a row-wise basis.\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define a list of integers\nnumbers <- list(1, 2, 3, 4, 5)\n\n# define a function to apply\nsquare <- function(x) {\n  x * x\n}\n\n# apply the function over list using spark\nspark.lapply(numbers, square)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a spark DataFrame of given length\nsdf <- sdf_len(sc, 5, repartition = 1)\n\n# apply function to each partition of data.frame\nspark_apply(sdf, f = nrow) # <1>\n\n# apply function to each row (option 1)\nspark_apply(sdf, f = nrow, group_by = \"id\") # <2>\n\n# apply function to each row (option 2)\nrow_func <- function(df) { # <3>\n  df |> # <3>\n    dplyr::rowwise() |> # <3>\n    dplyr::mutate(x = id * 2)\n} # <3>\nspark_apply(sdf, f = row_func) # <3>\n```\n:::\n\n\n\n\n\n\n\n\n1.  `spark_apply()` defaults to processing data based on number of partitions, in this case it will return a single row as due to `repartition = 1`.\n2.  To force behaviour like `spark.lapply()` you can create a DataFrame with `N` rows and force grouping with `group_by` set to a unique row identifier (in this case it's the `id` column automatically generated by `sdf_len()`). This will return `N` rows.\n3.  This requires writing a function that operates across rows of a `data.frame`, in some occassions this *may* be faster relative to (2). Specifying `group_by` in optional for this example. This example does not require `rowwise()`, but is just to illustrate one method to force computations to be for every row. Your function should take care to import required packages, etc.\n\n## Machine learning\n\nFull examples for each package can be found in the official reference for [`{SparkR}`](https://spark.apache.org/docs/3.5.2/ml-guide.html) and [`{sparklyr}`](https://spark.posit.co/packages/sparklyr/latest/reference/#spark-ml---regression), respectively.\n\nIf not using Spark MLlib it is recommended to use UDFs to train with the library of your choice (e.g. `{xgboost}`).\n\n### Linear regression\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select features\ntraining_df <- select(mtcars_df, \"mpg\", \"hp\", \"wt\")\n\n# fit the model using Generalized Linear Model (GLM)\nlinear_model <- spark.glm(training_df, mpg ~ hp + wt, family = \"gaussian\")\n\n# view model summary\nsummary(linear_model)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# select features\ntraining_tbl <- mtcars_tbl |>\n  select(mpg, hp, wt)\n\n# fit the model using Generalized Linear Model\nlinear_model <- training_tbl |>\n  ml_linear_regression(response = \"mpg\", features = c(\"hp\", \"wt\"))\n\n# view model summary\nsummary(linear_model)\n```\n:::\n\n\n\n\n\n\n\n\n### K-means clustering\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# apply KMeans clustering with 3 clusters using mpg and hp as features\nkmeans_model <- spark.kmeans(mtcars_df, mpg ~ hp, k = 3)\n\n# get cluster predictions\npredict(kmeans_model, mtcars_df) # <1>\n```\n:::\n\n\n\n\n\n\n\n\n1.  Predicting on input data to keep example simple\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# use mpg and hp as features\nfeatures_tbl <- mtcars_tbl |>\n  select(mpg, hp)\n\n# assemble features into a vector column\nfeatures_vector_tbl <- features_tbl |>\n  ft_vector_assembler(\n    input_cols = c(\"mpg\", \"hp\"),\n    output_col = \"features\"\n  )\n\n# apply K-Means clustering\nkmeans_model <- features_vector_tbl |>\n  ml_kmeans(features_col = \"features\", k = 3)\n\n# get cluster predictions\nml_predict(kmeans_model, features_vector_tbl) # <1>\n```\n:::\n\n\n\n\n\n\n\n\n1.  Predicting on input data to keep example simple\n\n## Performance and optimization\n\n### Collecting\n\nBoth `{SparkR}` and `{sparklyr}` use the same function name, `collect()`, to convert Spark DataFrames to R data frames. In general, only collect small amounts of data back to R data frames or the Spark driver will run out of memory, crashing your script (and you want to use Spark to accelerate workloads as much as possible!).\n\nTo prevent out of memory errors, `{SparkR}` has built-in optimizations in Databricks Runtime that help collect data or execute user-defined functions (which also require collecting data to workers). To ensure smooth performance with `{sparklyr}` for collecting data and UDFs, make sure to load the [`{arrow}`](https://arrow.apache.org/docs/r/) package in your scripts.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# when on Databricks DBR 14.3 or higher {arrow} is pre-installed\nlibrary(arrow)\n```\n:::\n\n\n\n\n\n\n\n\nIf you encounter issues with collecting ***large*** datasets with `{sparklyr}` the methods documented [here](https://medium.com/@NotZacDavies/collecting-large-results-with-sparklyr-8256a0370ec6) may assist, however, hitting this is typically an indicator that you should defer more work to Spark.\n\n### In-Memory Partitioning\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# repartition the SparkDataFrame based on 'cyl' column\nrepartition(mtcars_df, col = mtcars_df$cyl)\n\n# repartition the SparkDataFrame to number of partitions\nrepartition(mtcars_df, numPartitions = 10)\n\n# coalesce the SparkDataFrame to number of partitions\ncoalesce(mtcars_df, numPartitions = 1)\n\n# get number of partitions\ngetNumPartitions(mtcars_df)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# repartition the tbl_spark based on 'cyl' column\nsdf_repartition(mtcars_tbl, partition_by = \"cyl\")\n\n# repartition the tbl_spark to number of partitions\nsdf_repartition(mtcars_tbl, partitions = 10)\n\n# coalesce the tbl_spark to number of partitions\nsdf_coalesce(mtcars_tbl, partitions = 1)\n\n# get number of partitions\nsdf_num_partitions(mtcars_tbl)\n```\n:::\n\n\n\n\n\n\n\n\n### Caching\n\n**SparkR**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cache the SparkDataFrame in memory\ncache(mtcars_df)\n```\n:::\n\n\n\n\n\n\n\n\n**sparklyr**\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# cache the tbl_spark in memory\ntbl_cache(sc, name = \"mtcars_tmp\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}