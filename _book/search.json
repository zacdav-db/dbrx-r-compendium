[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R on Databricks Compendium",
    "section": "",
    "text": "What is this?\n\n\n\n\n\n\nUnder Development\n\n\n\n\n\n\n\n\n\n\n\n\nThis is not intended to be an exhaustive guide, it‚Äôs currently a place for me to document and collate useful information regarding R on Databricks.\n\n\n\n\n\n\nThere aren‚Äôt many definitive examples of how to use R and Databricks together - hopefully the content here will serve as a useful resource."
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#unity-catalog-model-requirements",
    "href": "chapters/mlflow/log-to-uc.html#unity-catalog-model-requirements",
    "title": "1¬† Log R Models to Unity Catalog",
    "section": "1.1 Unity Catalog Model Requirements",
    "text": "1.1 Unity Catalog Model Requirements\nFor models to be logged into Unity Catalog they must have a model signature. The Model signature defines the schema for model inputs/outputs.\nTypically when using python this would be inferred via model input examples. Input examples are optional but strongly recommended.\nThe documentation discusses signature enforcement, currently this isn‚Äôt implemented for R. Therefore you can decide if the signature is a dummy value for the sake of moving forward, or correct to clearly communicate the behaviour of the model.\n\n\n\n\n\n\nImportant\n\n\n\nIt‚Äôs important to clarify that for python the signature is enforced at time of inference not when registering the model to Unity Catalog.\nThe signature correctness is not validated when registering the model, it just has to be syntactically valid.\n\n\nSo, let‚Äôs look at the existing code to log models in the crate flavour:\n\nmlflow_save_model.crate &lt;- function(model, path, model_spec=list(), ...) {\n1  if (dir.exists(path)) unlink(path, recursive = TRUE)\n  dir.create(path)\n\n2  serialized &lt;- serialize(model, NULL)\n\n3  saveRDS(\n    serialized,\n    file.path(path, \"crate.bin\")\n  )\n\n4  model_spec$flavors &lt;- append(model_spec$flavors, list(\n    crate = list(\n      version = \"0.1.0\",\n      model = \"crate.bin\"\n    )\n  ))\n  mlflow_write_model_spec(path, model_spec)\n  model_spec\n}\n\n\n1\n\nCreate the directory to save the model if it doesn‚Äôt exist, if it does, empty it\n\n2\n\nSerialise the model, which is an object of class crate (from {carrier} package)\n\n3\n\nSave the serialised model via saveRDS to the directory as crate.bin\n\n4\n\nDefine the model specification, this contains metadata required ensure reproducibility. In this case it‚Äôs only specifying a version and what file the model can be found within.\n\n\n\n\nThe missing puzzle piece is the definition of a signature. Instead of explicitly adding code to the crate flavour itself, we‚Äôll take advantage of the model_spec parameter.\nThat means we can focus on mlflow::mlflow_log_model directly, we‚Äôd need to adjust the code as follows:\n\n1mlflow_log_model &lt;- function(model, artifact_path, ...) {\n  \n  temp_path &lt;- fs::path_temp(artifact_path)\n  \n  model_spec &lt;- mlflow_save_model(\n    model, path = temp_path,\n2    model_spec = list(\n      utc_time_created = mlflow_timestamp(),\n      run_id = mlflow_get_active_run_id_or_start_run(),\n      artifact_path = artifact_path,\n      flavors = list()\n    ),\n  ...)\n  \n  res &lt;- mlflow_log_artifact(path = temp_path, artifact_path = artifact_path)\n  \n  tryCatch({\n    mlflow:::mlflow_record_logged_model(model_spec)\n  },\n  error = function(e) {\n    warning(\n      paste(\"Logging model metadata to the tracking server has failed, possibly due to older\",\n            \"server version. The model artifacts have been logged successfully.\",\n            \"In addition to exporting model artifacts, MLflow clients 1.7.0 and above\",\n            \"attempt to record model metadata to the  tracking store. If logging to a\",\n            \"mlflow server via REST, consider  upgrading the server version to MLflow\",\n            \"1.7.0 or above.\", sep=\" \")\n    )\n  })\n  res\n}\n\n\n1\n\nAdd a new parameter signature\n\n2\n\nPropagate signature to the model_spec parameter when invoking mlflow::mlflow_save_model\n\n\n\n\nBenefit of this method is that all model flavors will inherit the capability to log a signature."
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#working-through-the-solution",
    "href": "chapters/mlflow/log-to-uc.html#working-through-the-solution",
    "title": "1¬† Log R Models to Unity Catalog",
    "section": "1.2 Working Through the Solution",
    "text": "1.2 Working Through the Solution\nTo keep things simple we‚Äôll be logging a ‚Äúmodel‚Äù (a function which divides by two).\n\nhalf &lt;- function(x) x / 2\n\nhalf(1:10)\n\n [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nWithout any changes, a simplified example of logging to {mlflow} would look like:\n\nlibrary(carrier)\nlibrary(mlflow)\n\nwith(mlflow_start_run(), {\n  # typically you'd do more modelling related activities here\n  model &lt;- carrier::crate(~half(.x))\n1  mlflow_log_model(model, \"model\")\n})\n\n\n1\n\nAs discussed earlier, this is where things start to go awry with respect to Unity Catalog\n\n\n\n\n\n1.2.1 Patching mlflow_log_model\n\n\n\n\n\n\nNote\n\n\n\nTechnically, patching mlflow_log_model isn‚Äôt the only way to achieve this fix - you could modify the yaml after it‚Äôs written.\nI won‚Äôt be showing that method as It‚Äôs just as tedious and can change depending on the model flavour (with respect to where artifacts may reside), patching is more robust.\n\n\n\n1mlflow_log_model &lt;- function(model, artifact_path, signature = NULL, ...) {\n  \n2  format_signature &lt;- function(signature) {\n    lapply(signature, function(x) {\n      jsonlite::toJSON(x, auto_unbox = TRUE)\n    })\n  }\n  \n  temp_path &lt;- fs::path_temp(artifact_path)\n  \n  model_spec &lt;- mlflow_save_model(model, path = temp_path, model_spec = list(\n    utc_time_created = mlflow:::mlflow_timestamp(),\n    run_id = mlflow:::mlflow_get_active_run_id_or_start_run(),\n    artifact_path = artifact_path, \n    flavors = list(),\n3    signature = format_signature(signature)\n  ), ...)\n  \n  res &lt;- mlflow_log_artifact(path = temp_path, artifact_path = artifact_path)\n  \n  tryCatch({\n    mlflow:::mlflow_record_logged_model(model_spec)\n  },\n  error = function(e) {\n    warning(\n      paste(\"Logging model metadata to the tracking server has failed, possibly due to older\",\n            \"server version. The model artifacts have been logged successfully.\",\n            \"In addition to exporting model artifacts, MLflow clients 1.7.0 and above\",\n            \"attempt to record model metadata to the  tracking store. If logging to a\",\n            \"mlflow server via REST, consider  upgrading the server version to MLflow\",\n            \"1.7.0 or above.\", sep=\" \")\n    )\n  })\n  res\n}\n\n# overriding the function in the existing mlflow namespace \nassignInNamespace(\"mlflow_log_model\", mlflow_log_model, ns = \"mlflow\")\n\n\n1\n\nsignature has been added to function parameters, it‚Äôs defaulting to NULL so that existing code won‚Äôt break\n\n2\n\nAdding format_signature function so don‚Äôt need to write JSON by hand, adding this within function for simplicity\n\n3\n\nsignature is propagated to mlflow_save_model‚Äôs model_spec parameter which will write a valid signature\n\n\n\n\n\n\n1.2.2 Logging Model with a Signature\n\nwith(mlflow_start_run(), {\n  # typically you'd do more modelling related activities here\n  model &lt;- carrier::crate(~half(.x))\n1  signature &lt;- list(\n    inputs = list(list(type = \"double\", name = \"x\")),\n    outputs = list(list(type = \"double\"))\n  )\n2  mlflow_log_model(model, \"model\", signature = signature)\n})\n\n\n1\n\nExplicitly defining a signature, a list that contains input and outputs, each are lists of lists respectively\n\n2\n\nPassing defined signature to the now patched mlflow_log_model function\n\n\n\n\n\n\n1.2.3 Registration to Unity Catalog\nNow that the prerequisite of adding a model signature has been satisfied there is one last hurdle to overcome, registering to Unity Catalog.\nThe hurdle is due to {mlflow} not having been updated yet to support registration to Unity Catalog directly. The easiest way to overcome this is to simply register the run via python.\nFor example:\n\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\n\ncatalog = \"main\"\nschema = \"default\"\nmodel_name = \"my_model\"\n1run_uri = \"runs:/&lt;run_id&gt;/model\"\n\nmlflow.register_model(run_uri, f\"{catalog}.{schema}.{model_name}\")\n\n\n1\n\nYou‚Äôll need to either get the run_uri programmatically or copy it manually\n\n\n\n\nTo do this with R you‚Äôll need to make a series of requests to Unity Catalog endpoints for registering model, the specific steps are:\n\n(Optional) Create a new model in Unity Catalog\n\nPOST request on /api/2.0/mlflow/unity-catalog/registered-models/create\n\nname: 3 tiered namespace (e.g.¬†main.default.my_model)\n\n\nCreate a version for the model\n\nPOST request on /api/2.0/mlflow/unity-catalog/model-versions/create\n\nname: 3 tiered namespace (e.g.¬†main.default.my_model)\nsource: URI indicating the location of the model artifacts\nrun_id: run_id from tracking server that generated the model\nrun_tracking_server_id: Workspace ID of run that generated the model\n\nThis will return storage_location and version\n\nCopy model artifacts\n\nNeed to copy the artifacts to storage_location from step (2)\n\nFinalise the model version\n\nPOST request on /api/2.0/mlflow/unity-catalog/model-versions/finalize\n\nname: 3 tiered namespace (e.g.¬†main.default.my_model)\nversion: version returned from step (2)\n\n\n\nIt‚Äôs considerably easier to just use Python to register the model at this time."
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#fixing-mlflow",
    "href": "chapters/mlflow/log-to-uc.html#fixing-mlflow",
    "title": "1¬† Log R Models to Unity Catalog",
    "section": "1.3 Fixing mlflow",
    "text": "1.3 Fixing mlflow\nIdeally this page wouldn‚Äôt exist and {mlflow} would support Unity Catalog. Hopefully sometime soon I find the time to make a pull request myself - until then this serves as a guide."
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html",
    "href": "chapters/pkg-management/fast-installs.html",
    "title": "3¬† Faster Package Installs",
    "section": "",
    "text": "Under Development\n\n\n\n\n\n\nYou may have noticed that when installing packages in the notebook it can take a while. It could be minutes, hours in extreme cases, to install the suite of packages your project requires. This is especially tedious if you need to do this every time a job runs or each morning when your cluster is started (clusters are ephemeral and by default have no persistent storage).\nThis is because by default Databricks installs packages from CRAN. CRAN does not provide pre-compiled binaries for Linux (and Databricks clusters underlying virtual machines are all Linux, Ubuntu specifically).\nPosit to save the day! Posit provides a public package manager that has all packages from CRAN (and Bioconductor!). There is a helpful wizard to get started.\nWith our new found knowledge we can make installing R packages within Databricks significantly faster. There are three ways to solve this, each differing slightly, but fundamentally the same."
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html",
    "href": "chapters/pkg-management/persisting-libs.html",
    "title": "4¬† Persisting Packages",
    "section": "",
    "text": "Under Development"
  },
  {
    "objectID": "chapters/pkg-management/renv.html",
    "href": "chapters/pkg-management/renv.html",
    "title": "5¬† {renv}",
    "section": "",
    "text": "Under Development"
  },
  {
    "objectID": "chapters/misc/htmlwidgets.html",
    "href": "chapters/misc/htmlwidgets.html",
    "title": "7¬† {htmlwidgets} in notebooks",
    "section": "",
    "text": "When you try to view a {htmlwidget} based visualisation (e.g.¬†{leaflet}) in a Databricks notebook you‚Äôll find there is no rendered output by default.\nThe Databricks documentation details how to get this working but requires specification of the workspace URL explicitly.\n{brickster} has a helper function that simplifies enabling {htmlwidgets}:\n\nremotes::install_github(\"zacdav-db/brickster\")\nbrickster::notebook_enable_htmlwidgets()\n\nThe function itself is straightforward, here is code (simplified version of brickster::notebook_enable_htmlwidgets) that doesn‚Äôt require installing {brickster}:\n\nenable_htmlwidgets &lt;- function(height = 450) {\n\n  # new option to control default widget height, default is 450px\n1  options(db_htmlwidget_height = height)\n\n2  system(\"apt-get --yes install pandoc\", intern = T)\n3  if (!base::require(\"htmlwidgets\")) {\n    utils::install.packages(\"htmlwidgets\")\n  }\n\n  # new method will fetch height based on new option, or default to 450px\n4  new_method &lt;- function(x, ...) {\n    x$height &lt;- getOption(\"db_htmlwidget_height\", 450)\n    file &lt;- tempfile(fileext = \".html\")\n    htmlwidgets::saveWidget(x, file = file)\n    contents &lt;- as.character(rvest::read_html(file))\n    displayHTML(contents)\n  }\n\n5  utils::assignInNamespace(\"print.htmlwidget\", new_method, ns = \"htmlwidgets\")\n  invisible(list(default_height = height, print = new_method))\n  \n}\n\n\n1\n\nThe height of the htmlwidget output is controlled via an option (db_htmlwidget_height), this allows the height to be adjusted without re-running the function\n\n2\n\nInstalling pandoc as it‚Äôs required to use htmlwidgets::saveWidget\n\n3\n\nEnsure that {htmlwidgets} is installed\n\n4\n\nFunction that writes the widget to a temporary file as a self-contained html file and then reads the contents and presents via displayHTML\n\n5\n\nOverride the htmlwidgets::print.htmlwidget method"
  },
  {
    "objectID": "chapters/mlflow/model-serving.html",
    "href": "chapters/mlflow/model-serving.html",
    "title": "2¬† Model Serving",
    "section": "",
    "text": "Under Development"
  },
  {
    "objectID": "chapters/misc/odbc-oauth.html",
    "href": "chapters/misc/odbc-oauth.html",
    "title": "8¬† OAuth ü§ù {odbc}",
    "section": "",
    "text": "Under Development"
  },
  {
    "objectID": "chapters/data-eng/odbc-dbplyr.html",
    "href": "chapters/data-eng/odbc-dbplyr.html",
    "title": "6¬† {dbplyr} & {odbc}",
    "section": "",
    "text": "Under Development"
  },
  {
    "objectID": "chapters/data-eng/odbc-dbplyr.html#u2m-example",
    "href": "chapters/data-eng/odbc-dbplyr.html#u2m-example",
    "title": "6¬† {dbplyr} & {odbc}",
    "section": "6.1 U2M Example",
    "text": "6.1 U2M Example\n\n\n\n\n\n\nNote\n\n\n\nOAuth U2M or OAuth 2.0 browser-based authentication works only with applications that run locally. It does not work with server-based or cloud-based applications.\n\n\n\nlibrary(odbc)\nlibrary(DBI)\n\ncon &lt;- DBI::dbConnect(\n1  drv = odbc::databricks(),\n2  httpPath = \"/sql/1.0/warehouses/&lt;warehouse-id&gt;\",\n3  workspace = \"&lt;workspace-name&gt;.cloud.databricks.com\",\n4  authMech = 11,\n  auth_flow = 2\n)\n\n\n1\n\n{odbc} recently added odbc::databricks() to simplify connecting to Databricks (requires version &gt;=1.4.0)\n\n2\n\nThe httpPath can be found in the ‚ÄòConnection Details‚Äô tab of a SQL warehouse\n\n3\n\nworkspace refers to the workspace URL, also found in ‚ÄòConnection Details‚Äô tab as ‚ÄòServer hostname‚Äô\n\n4\n\nThe docs mention setting AuthMech to 11 and Auth_Flow to 2"
  },
  {
    "objectID": "chapters/misc/odbc-oauth.html#u2m-example",
    "href": "chapters/misc/odbc-oauth.html#u2m-example",
    "title": "8¬† OAuth ü§ù {odbc}",
    "section": "8.1 U2M Example",
    "text": "8.1 U2M Example\n\n\n\n\n\n\nNote\n\n\n\nOAuth U2M or OAuth 2.0 browser-based authentication works only with applications that run locally. It does not work with server-based or cloud-based applications.\n\n\nWhen running this code you should be prompted to login to the workspace or you‚Äôll see a window that says ‚Äúsuccess‚Äù. You can close the window and continue working in R.\n\nlibrary(odbc)\nlibrary(DBI)\n\ncon &lt;- DBI::dbConnect(\n1  drv = odbc::databricks(),\n2  httpPath = \"/sql/1.0/warehouses/&lt;warehouse-id&gt;\",\n3  workspace = \"&lt;workspace-name&gt;.cloud.databricks.com\",\n4  authMech = 11,\n  auth_flow = 2\n)\n\n\n1\n\n{odbc} recently added odbc::databricks() to simplify connecting to Databricks (requires version &gt;=1.4.0)\n\n2\n\nThe httpPath can be found in the ‚ÄòConnection Details‚Äô tab of a SQL warehouse\n\n3\n\nworkspace refers to the workspace URL, also found in ‚ÄòConnection Details‚Äô tab as ‚ÄòServer hostname‚Äô\n\n4\n\nThe docs mention setting AuthMech to 11 and Auth_Flow to 2"
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#notebook-one-liner",
    "href": "chapters/pkg-management/fast-installs.html#notebook-one-liner",
    "title": "3¬† Faster Package Installs",
    "section": "3.1 Notebook one-liner",
    "text": "3.1 Notebook one-liner"
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#section",
    "href": "chapters/pkg-management/fast-installs.html#section",
    "title": "3¬† Faster Package Installs",
    "section": "3.4 ",
    "text": "3.4"
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#setting-repo-within-notebook",
    "href": "chapters/pkg-management/fast-installs.html#setting-repo-within-notebook",
    "title": "3¬† Faster Package Installs",
    "section": "3.1 Setting Repo within Notebook",
    "text": "3.1 Setting Repo within Notebook\nThe quickest method is to follow the wizard and adjust the repos option:\n\n# need to set the user agent string otherwise installs will be slow\n# e.g. selecting Ubuntu 22.04 in wizard \noptions(\n  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])),\n  repos = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\"\n)\n\nThis works well but not all versions of the Databricks Runtime use the same version of Ubuntu.\nIt‚Äôs easier to detect the Ubuntu release code name dynamically:\n\n1release &lt;- system(\"lsb_release -c --short\", intern = T)\n\n# need to set the user agent string otherwise installs will be slow\noptions(\n  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])),\n  repos = paste0(\"https://packagemanager.posit.co/cran/__linux__/\", release, \"/latest\")\n)\n\n\n1\n\nsystem is used to run the command to retrieve the release code name\n\n\n\n\nThe downside of this method is that it requires every notebook to adjust the repos and HTTPUserAgent options."
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#cluster-settings",
    "href": "chapters/pkg-management/fast-installs.html#cluster-settings",
    "title": "3¬† Faster Package Installs",
    "section": "3.2 Cluster Settings",
    "text": "3.2 Cluster Settings"
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#adjusting-cluster-settings",
    "href": "chapters/pkg-management/fast-installs.html#adjusting-cluster-settings",
    "title": "3¬† Faster Package Installs",
    "section": "3.2 Adjusting Cluster Settings",
    "text": "3.2 Adjusting Cluster Settings\nDatabricks clusters allow specification of environment variables, there is a specific variable (DATABRICKS_DEFAULT_R_REPOS) that can be set to adjust the default repository for the entire cluster.\nUnfortunately this isn‚Äôt as dynamic as the first option. The Ubuntu version used doesn‚Äôt change often, so it‚Äôs not a big issue\nYou can again refer to the wizard, your environment variables section of cluster should have:\n\n# be sure to use the correct URL\nDATABRICKS_DEFAULT_R_REPOS=&lt;posit-package-manager-url-goes-here&gt;"
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#setting-repo-for-cluster-library",
    "href": "chapters/pkg-management/fast-installs.html#setting-repo-for-cluster-library",
    "title": "3¬† Faster Package Installs",
    "section": "3.3 Setting Repo for Cluster Library",
    "text": "3.3 Setting Repo for Cluster Library\n\n\n\n\n\n\nNote\n\n\n\nSimilar to setting DATABRICKS_DEFAULT_R_REPOS this requires the HTTPUserAgent also to be set and it‚Äôs unlikely to be helpful other than for it‚Äôs purpose of installing a package to make it available for all cluster users.\n\n\nCluster libraries can install R packages and support specification of the repository."
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#init-script",
    "href": "chapters/pkg-management/fast-installs.html#init-script",
    "title": "3¬† Faster Package Installs",
    "section": "3.4 Init Script",
    "text": "3.4 Init Script\n\n\n#"
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#adjusting-cluster-environment-variable",
    "href": "chapters/pkg-management/fast-installs.html#adjusting-cluster-environment-variable",
    "title": "3¬† Faster Package Installs",
    "section": "3.2 Adjusting Cluster Environment Variable",
    "text": "3.2 Adjusting Cluster Environment Variable\nDatabricks clusters allow specification of environment variables, there is a specific variable (DATABRICKS_DEFAULT_R_REPOS) that can be set to adjust the default repository for the entire cluster.\nUnfortunately this isn‚Äôt as dynamic as the first option. The Ubuntu version used doesn‚Äôt change often, so it‚Äôs not a big issue\nYou can again refer to the wizard, your environment variables section of cluster should have:\n\n# be sure to use the correct URL\nDATABRICKS_DEFAULT_R_REPOS=&lt;posit-package-manager-url-goes-here&gt;"
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#cluster-environment-variable",
    "href": "chapters/pkg-management/fast-installs.html#cluster-environment-variable",
    "title": "3¬† Faster Package Installs",
    "section": "3.2 Cluster Environment Variable",
    "text": "3.2 Cluster Environment Variable\nDatabricks clusters allow specification of environment variables, there is a specific variable (DATABRICKS_DEFAULT_R_REPOS) that can be set to adjust the default repository for the entire cluster.\nUnfortunately this isn‚Äôt as dynamic as the first option. The Ubuntu version used doesn‚Äôt change often, so it‚Äôs not a big issue\nYou can again refer to the wizard, your environment variables section of cluster should have:\n\n# be sure to use the correct URL\nDATABRICKS_DEFAULT_R_REPOS=&lt;posit-package-manager-url-goes-here&gt;"
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#cluster-environment-variable-init-script",
    "href": "chapters/pkg-management/fast-installs.html#cluster-environment-variable-init-script",
    "title": "3¬† Faster Package Installs",
    "section": "3.2 Cluster Environment Variable & Init Script",
    "text": "3.2 Cluster Environment Variable & Init Script\nDatabricks clusters allow specification of environment variables, there is a specific variable (DATABRICKS_DEFAULT_R_REPOS) that can be set to adjust the default repository for the entire cluster.\nYou can again refer to the wizard, the environment variables section of cluster should be:\n\nDATABRICKS_DEFAULT_R_REPOS=&lt;posit-package-manager-url-goes-here&gt;\n\nUnfortunately this isn‚Äôt as dynamic as the first option and you still need to set the HTTPUserAgent in Rprofile.site via an init script.\nThe init script will be:\n\n#!/bin/bash\n# Append changes to Rprofile.site\ncat &lt;&lt;EOF &gt;&gt; \"/etc/R/Rprofile.site\"\noptions(\n  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"]))\n)\nEOF\n\n\n\n\n\n\n\nImportant\n\n\n\nDue to how Databricks starts up the R shell for notebook sessions it‚Äôs not straightforward to adjust the repos option in an init script alone.\nDATABRICKS_DEFAULT_R_REPOS is referenced as part of the startup process after Rprofile.site is executed and will override any earlier attempt to adjust repos.\nTherefore you‚Äôll need to use both the init script and the environment variable configuration."
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#section",
    "href": "chapters/mlflow/model-serving.html#section",
    "title": "2¬† Model Serving",
    "section": "2.1 ",
    "text": "2.1"
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#model-flavours",
    "href": "chapters/mlflow/model-serving.html#model-flavours",
    "title": "2¬† Model Serving",
    "section": "2.1 Model Flavours",
    "text": "2.1 Model Flavours"
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#xgboost",
    "href": "chapters/mlflow/model-serving.html#xgboost",
    "title": "2¬† Model Serving",
    "section": "2.2 XGBoost",
    "text": "2.2 XGBoost\n\n\n\n\n\n\nCaution\n\n\n\nWork In Progress\n\n\nXGBoost is relatively straightforward since it the artifacts generated when saving the model are universally understood across each language the library is available in."
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#crate",
    "href": "chapters/mlflow/model-serving.html#crate",
    "title": "2¬† Model Serving",
    "section": "2.4 Crate",
    "text": "2.4 Crate\n\n\n\n\n\n\nCaution\n\n\n\nWork In Progress"
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#onnx",
    "href": "chapters/mlflow/model-serving.html#onnx",
    "title": "2¬† Model Serving",
    "section": "2.3 ONNX",
    "text": "2.3 ONNX\n\n\n\n\n\n\nCaution\n\n\n\nWork In Progress"
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#understanding-serving",
    "href": "chapters/mlflow/model-serving.html#understanding-serving",
    "title": "2¬† Model Serving",
    "section": "2.1 Understanding Serving",
    "text": "2.1 Understanding Serving"
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#serving-overview",
    "href": "chapters/mlflow/model-serving.html#serving-overview",
    "title": "2¬† Model Serving",
    "section": "2.1 Serving Overview",
    "text": "2.1 Serving Overview\n\n\n\n\n\n\nCaution\n\n\n\nWork In Progress"
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#serving-101",
    "href": "chapters/mlflow/model-serving.html#serving-101",
    "title": "2¬† Model Serving",
    "section": "2.1 Serving 101",
    "text": "2.1 Serving 101"
  }
]