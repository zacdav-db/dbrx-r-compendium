[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R on Databricks Compendium",
    "section": "",
    "text": "What is this?\n\n\n\n\n\n\nUnder Development\n\n\n\n\n\n\n\n\n\n\n\n\nThis is not intended to be an exhaustive guide, it’s currently a place for me to document and collate useful information regarding R on Databricks.\n\n\n\n\n\n\nThere aren’t many definitive examples of how to use R and Databricks together - hopefully the content here will serve as a useful resource.",
    "crumbs": [
      "What is this?"
    ]
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html",
    "href": "chapters/mlflow/log-to-uc.html",
    "title": "1  Log R Models to Unity Catalog",
    "section": "",
    "text": "1.1 Unity Catalog Model Requirements\nCurrently {mlflow} doesn’t support directly logging R models to Unity Catalog. This section will cover why, and then how to overcome each roadblock.\nFor models to be logged into Unity Catalog they must have a model signature. The Model signature defines the schema for model inputs/outputs.\nTypically when using python this would be inferred via model input examples. Input examples are optional but strongly recommended.\nThe documentation discusses signature enforcement, currently this isn’t implemented for R. Therefore you can decide if the signature is a dummy value for the sake of moving forward, or correct to clearly communicate the behaviour of the model.\nSo, let’s look at the existing code to log models in the crate flavour:\nmlflow_save_model.crate &lt;- function(model, path, model_spec=list(), ...) {\n1  if (dir.exists(path)) unlink(path, recursive = TRUE)\n  dir.create(path)\n\n2  serialized &lt;- serialize(model, NULL)\n\n3  saveRDS(\n    serialized,\n    file.path(path, \"crate.bin\")\n  )\n\n4  model_spec$flavors &lt;- append(model_spec$flavors, list(\n    crate = list(\n      version = \"0.1.0\",\n      model = \"crate.bin\"\n    )\n  ))\n  mlflow_write_model_spec(path, model_spec)\n  model_spec\n}\n\n\n1\n\nCreate the directory to save the model if it doesn’t exist, if it does, empty it\n\n2\n\nSerialise the model, which is an object of class crate (from {carrier} package)\n\n3\n\nSave the serialised model via saveRDS to the directory as crate.bin\n\n4\n\nDefine the model specification, this contains metadata required ensure reproducibility. In this case it’s only specifying a version and what file the model can be found within.\nThe missing puzzle piece is the definition of a signature. Instead of explicitly adding code to the crate flavour itself, we’ll take advantage of the model_spec parameter.\nThat means we can focus on mlflow::mlflow_log_model directly, we’d need to adjust the code as follows:\n1mlflow_log_model &lt;- function(model, artifact_path, signature, ...) {\n  \n  temp_path &lt;- fs::path_temp(artifact_path)\n  \n  model_spec &lt;- mlflow_save_model(\n    model, path = temp_path,\n2    model_spec = list(\n      utc_time_created = mlflow_timestamp(),\n      run_id = mlflow_get_active_run_id_or_start_run(),\n      artifact_path = artifact_path,\n      flavors = list(),\n      signature = signature\n    ),\n  ...)\n  \n  res &lt;- mlflow_log_artifact(path = temp_path, artifact_path = artifact_path)\n  \n  tryCatch({\n    mlflow:::mlflow_record_logged_model(model_spec)\n  },\n  error = function(e) {\n    warning(\n      paste(\"Logging model metadata to the tracking server has failed, possibly due to older\",\n            \"server version. The model artifacts have been logged successfully.\",\n            \"In addition to exporting model artifacts, MLflow clients 1.7.0 and above\",\n            \"attempt to record model metadata to the  tracking store. If logging to a\",\n            \"mlflow server via REST, consider  upgrading the server version to MLflow\",\n            \"1.7.0 or above.\", sep=\" \")\n    )\n  })\n  res\n}\n\n\n1\n\nAdd a new parameter signature\n\n2\n\nPropagate signature to the model_spec parameter when invoking mlflow::mlflow_save_model\nBenefit of this method is that all model flavors will inherit the capability to log a signature.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Log R Models to Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#unity-catalog-model-requirements",
    "href": "chapters/mlflow/log-to-uc.html#unity-catalog-model-requirements",
    "title": "1  Log R Models to Unity Catalog",
    "section": "",
    "text": "Important\n\n\n\nIt’s important to clarify that for python the signature is enforced at time of inference not when registering the model to Unity Catalog.\nThe signature correctness is not validated when registering the model, it just has to be syntactically valid.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Log R Models to Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#working-through-the-solution",
    "href": "chapters/mlflow/log-to-uc.html#working-through-the-solution",
    "title": "1  Log R Models to Unity Catalog",
    "section": "1.2 Working Through the Solution",
    "text": "1.2 Working Through the Solution\nTo keep things simple we’ll be logging a “model” (a function which divides by two).\n\nhalf &lt;- function(x) x / 2\n\nhalf(1:10)\n\n [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nWithout any changes, a simplified example of logging to {mlflow} would look like:\n\nlibrary(carrier)\nlibrary(mlflow)\n\nwith(mlflow_start_run(), {\n  # typically you'd do more modelling related activities here\n  model &lt;- carrier::crate(~half(.x))\n1  mlflow_log_model(model, \"model\")\n})\n\n\n1\n\nAs discussed earlier, this is where things start to go awry with respect to Unity Catalog\n\n\n\n\n\n1.2.1 Patching mlflow_log_model\n\n\n\n\n\n\nNote\n\n\n\nTechnically, patching mlflow_log_model isn’t the only way to achieve this fix - you could modify the yaml after it’s written.\nI won’t be showing that method as It’s just as tedious and can change depending on the model flavour (with respect to where artifacts may reside), patching is more robust.\n\n\n\n1mlflow_log_model &lt;- function(model, artifact_path, signature = NULL, ...) {\n  \n2  format_signature &lt;- function(signature) {\n    lapply(signature, function(x) {\n      jsonlite::toJSON(x, auto_unbox = TRUE)\n    })\n  }\n  \n  temp_path &lt;- fs::path_temp(artifact_path)\n  \n  model_spec &lt;- mlflow_save_model(model, path = temp_path, model_spec = list(\n    utc_time_created = mlflow:::mlflow_timestamp(),\n    run_id = mlflow:::mlflow_get_active_run_id_or_start_run(),\n    artifact_path = artifact_path, \n    flavors = list(),\n3    signature = format_signature(signature)\n  ), ...)\n  \n  res &lt;- mlflow_log_artifact(path = temp_path, artifact_path = artifact_path)\n  \n  tryCatch({\n    mlflow:::mlflow_record_logged_model(model_spec)\n  },\n  error = function(e) {\n    warning(\n      paste(\"Logging model metadata to the tracking server has failed, possibly due to older\",\n            \"server version. The model artifacts have been logged successfully.\",\n            \"In addition to exporting model artifacts, MLflow clients 1.7.0 and above\",\n            \"attempt to record model metadata to the  tracking store. If logging to a\",\n            \"mlflow server via REST, consider  upgrading the server version to MLflow\",\n            \"1.7.0 or above.\", sep=\" \")\n    )\n  })\n  res\n}\n\n# overriding the function in the existing mlflow namespace \nassignInNamespace(\"mlflow_log_model\", mlflow_log_model, ns = \"mlflow\")\n\n\n1\n\nsignature has been added to function parameters, it’s defaulting to NULL so that existing code won’t break\n\n2\n\nAdding format_signature function so don’t need to write JSON by hand, adding this within function for simplicity\n\n3\n\nsignature is propagated to mlflow_save_model’s model_spec parameter which will write a valid signature\n\n\n\n\n\n\n1.2.2 Logging Model with a Signature\n\nwith(mlflow_start_run(), {\n  # typically you'd do more modelling related activities here\n  model &lt;- carrier::crate(~half(.x))\n1  signature &lt;- list(\n    inputs = list(list(type = \"double\", name = \"x\")),\n    outputs = list(list(type = \"double\"))\n  )\n2  mlflow_log_model(model, \"model\", signature = signature)\n})\n\n\n1\n\nExplicitly defining a signature, a list that contains input and outputs, each are lists of lists respectively\n\n2\n\nPassing defined signature to the now patched mlflow_log_model function\n\n\n\n\n\n\n1.2.3 Registration to Unity Catalog\nNow that the prerequisite of adding a model signature has been satisfied there is one last hurdle to overcome, registering to Unity Catalog.\nThe hurdle is due to {mlflow} not having been updated yet to support registration to Unity Catalog directly. The easiest way to overcome this is to simply register the run via python.\nFor example:\n\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\n\ncatalog = \"main\"\nschema = \"default\"\nmodel_name = \"my_model\"\n1run_uri = \"runs:/&lt;run_id&gt;/model\"\n\nmlflow.register_model(run_uri, f\"{catalog}.{schema}.{model_name}\")\n\n\n1\n\nYou’ll need to either get the run_uri programmatically or copy it manually\n\n\n\n\nTo do this with R you’ll need to make a series of requests to Unity Catalog endpoints for registering model, the specific steps are:\n\n(Optional) Create a new model in Unity Catalog\n\nPOST request on /api/2.0/mlflow/unity-catalog/registered-models/create\n\nname: 3 tiered namespace (e.g. main.default.my_model)\n\n\nCreate a version for the model\n\nPOST request on /api/2.0/mlflow/unity-catalog/model-versions/create\n\nname: 3 tiered namespace (e.g. main.default.my_model)\nsource: URI indicating the location of the model artifacts\nrun_id: run_id from tracking server that generated the model\nrun_tracking_server_id: Workspace ID of run that generated the model\n\nThis will return storage_location and version\n\nCopy model artifacts\n\nNeed to copy the artifacts to storage_location from step (2)\n\nFinalise the model version\n\nPOST request on /api/2.0/mlflow/unity-catalog/model-versions/finalize\n\nname: 3 tiered namespace (e.g. main.default.my_model)\nversion: version returned from step (2)\n\n\n\nIt’s considerably easier to just use Python to register the model at this time.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Log R Models to Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#fixing-mlflow",
    "href": "chapters/mlflow/log-to-uc.html#fixing-mlflow",
    "title": "1  Log R Models to Unity Catalog",
    "section": "1.3 Fixing mlflow",
    "text": "1.3 Fixing mlflow\nIdeally this page wouldn’t exist and {mlflow} would support Unity Catalog. Hopefully sometime soon I find the time to make a pull request myself - until then this serves as a guide.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Log R Models to Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/load-from-uc.html",
    "href": "chapters/mlflow/load-from-uc.html",
    "title": "2  Load R Models from Unity Catalog",
    "section": "",
    "text": "2.1 How?\n{mlflow} doesn’t support directly logging R models to Unity Catalog (without jumping through a few hoops). However, you can easily load models from Unity Catalog.\n# loading 'prod' alias of `zacdav`.`default`.`my_r_model`\nmodel &lt;- mlflow_load_model(\"models:/zacdav.default.my_r_model@prod\")\nYep, very straightforward, it just works as-is.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Load R Models from Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/load-from-uc.html#why-is-this-so-simple",
    "href": "chapters/mlflow/load-from-uc.html#why-is-this-so-simple",
    "title": "2  Load R Models from Unity Catalog",
    "section": "2.2 Why is this so simple?",
    "text": "2.2 Why is this so simple?\nA natural question is “why did I have to go through all the pain to register the model, but loading just works?”\nThe quick answer is that {mlflow} doesn’t have internal consistency with how it operates, some methods use direct API calls (e.g. registration) and where as others defer to the mlflow CLI via a system call (e.g. loading).\n\n2.2.1 The details\nAs of version 2.20.1 we can split {mlflow} into those that directly invokes mlflow’s REST API’s via R, or the mlflow CLI.\n\nWhich functions use REST or CLI clients within {mlflow}\n\n\n\n\n\n\n\n\nInvokes REST Client\nInvokes CLI Client\n\n\n\n\nNot relevant to Unity Catalog\n(e.g. experiment tracking related)\nmlflow_create_experiment()\nmlflow_search_experiments()\nmlflow_set_experiment_tag()\nmlflow_get_experiment()\nmlflow_delete_experiment()\nmlflow_delete_experiment()\nmlflow_rename_experiment()\nmlflow_transition_model_version_stage()\nmlflow_record_logged_model()\n\nmlflow_set_model_version_tag()\nmlflow_search_registered_models()\n\n\n\nRelevant to Unity Catalog\n(e.g. model registry)\nmlflow_create_registered_model()\nmlflow_get_registered_model()\nmlflow_rename_registered_model()\nmlflow_update_registered_model()\nmlflow_delete_registered_model()\nmlflow_get_latest_versions()\nmlflow_create_model_version()\nmlflow_get_model_version()\nmlflow_update_model_version()\nmlflow_log_model()*\nmlflow_download_artifacts_from_uri()\nmlflow_download_artifacts()\nmlflow_log_artifact()\nmlflow_load_model()\n\n\n\nmlflow_load_model() calls mlflow_download_artifacts_from_uri(), which in turn uses the CLI directly. The CLI was updated to support Unity Catalog, hence why it ‘just works’ and the functions inherit Unity Catalog compatibility without code changes.\nYou may notice mlflow_log_model() has an asterix, that is because it uniquely calls CLI client (mlflow_log_artifact()) and REST client (mlflow_record_logged_model()), I’ve placed it in the REST column for now.\nThis is part of the problem discussed previously in Section 1.2.3. Unfortunately the CLI doesn’t expose the model registry methods required to support registering models to Unity Catalog, these need to be implemented as part of the REST client in {mlflow}.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Load R Models from Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html",
    "href": "chapters/pkg-management/fast-installs.html",
    "title": "3  Faster Package Installs",
    "section": "",
    "text": "3.1 Setting Repo within Notebook\nYou may have noticed that when installing packages in the notebook it can take a while. It could be minutes, hours in extreme cases, to install the suite of packages your project requires. This is especially tedious if you need to do this every time a job runs, or each morning when your cluster is started.\nClusters are ephemeral and by default have no persistent storage, therefore installed packages will not be available on restart.\nBy default Databricks installs packages from CRAN. CRAN does not provide pre-compiled binaries for Linux (Databricks clusters’ underlying virtual machines are Linux, Ubuntu specifically).\nPosit to save the day! Posit provides a public package manager that has all packages from CRAN (and Bioconductor!). There is a helpful wizard to get started.\nWith our new found knowledge we can make installing R packages within Databricks significantly faster. There are multiple ways to solve this, each differing slightly, but fundamentally the same.\nThe quickest method is to follow the wizard and adjust the repos option:\n# set the user agent string otherwise pre-compiled binarys aren't used\n# e.g. selecting Ubuntu 22.04 in wizard \noptions(\n1  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])),\n  repos = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\"\n)\n\n\n1\n\nHTTPUserAgent is required when using R 3.6 or later\nThis works well but not all versions of the Databricks Runtime use the same version of Ubuntu.\nIt’s easier to detect the Ubuntu release code name dynamically:\n1release &lt;- system(\"lsb_release -c --short\", intern = T)\n\n# set the user agent string otherwise pre-compiled binarys aren't used\noptions(\n  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])),\n  repos = paste0(\"https://packagemanager.posit.co/cran/__linux__/\", release, \"/latest\")\n)\n\n\n1\n\nsystem is used to run the command to retrieve the release code name\nThe downside of this method is that it requires every notebook to adjust the repos and HTTPUserAgent options.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Faster Package Installs</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#cluster-environment-variable-init-script",
    "href": "chapters/pkg-management/fast-installs.html#cluster-environment-variable-init-script",
    "title": "3  Faster Package Installs",
    "section": "3.2 Cluster Environment Variable & Init Script",
    "text": "3.2 Cluster Environment Variable & Init Script\nDatabricks clusters allow specification of environment variables, there is a specific variable (DATABRICKS_DEFAULT_R_REPOS) that can be set to adjust the default repository for the entire cluster.\nYou can again refer to the wizard, the environment variables section of cluster should be:\n\nDATABRICKS_DEFAULT_R_REPOS=&lt;posit-package-manager-url-goes-here&gt;\n\nUnfortunately this isn’t as dynamic as the first option and you still need to set the HTTPUserAgent in Rprofile.site via an init script.\nThe init script will be:\n\n#!/bin/bash\n# Append changes to Rprofile.site\ncat &lt;&lt;EOF &gt;&gt; \"/etc/R/Rprofile.site\"\noptions(\n  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"]))\n)\nEOF\n\n\n\n\n\n\n\nImportant\n\n\n\nDue to how Databricks starts up the R shell for notebook sessions it’s not straightforward to adjust the repos option in an init script alone.\nDATABRICKS_DEFAULT_R_REPOS is referenced as part of the startup process after Rprofile.site is executed and will override any earlier attempt to adjust repos.\nTherefore you’ll need to use both the init script and the environment variable configuration.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Faster Package Installs</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#setting-repo-for-cluster-library",
    "href": "chapters/pkg-management/fast-installs.html#setting-repo-for-cluster-library",
    "title": "3  Faster Package Installs",
    "section": "3.3 Setting Repo for Cluster Library",
    "text": "3.3 Setting Repo for Cluster Library\n\n\n\n\n\n\nNote\n\n\n\nSimilar to setting DATABRICKS_DEFAULT_R_REPOS this requires the HTTPUserAgent also to be set and it’s unlikely to be helpful other than for it’s purpose of installing a package to make it available for all cluster users.\n\n\nCluster libraries can install R packages and support specification of the repository.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Faster Package Installs</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html",
    "href": "chapters/pkg-management/persisting-libs.html",
    "title": "4  Persisting Packages",
    "section": "",
    "text": "4.1 Where Packages are Installed\nDatabricks clusters are ephemeral and therefore any installed packages will not be available on restart. If the cluster has cluster libraries defined then those libraries are installed after the cluster is started - this can be time consuming when there are multiple packages.\nThe article on faster package installs details how to reduce the time it takes to install each package. Faster installs are great, but sometimes it’s preferable to not install at all and persist the packages required, similar to how you’d use R locally.\nWhen installing packages with install.packages the default behaviour is that they’ll be installed to the first element of .libPaths().\n.libPaths() returns the paths of “R library trees”, directories that R packages can reside. When you load a package it will be loaded from the first location it is found as dictated by .libPaths().\nWhen working within a Databricks notebook .libPaths() will return 6 values by default, in order they are:\nIt’s important to understand that the order defines the default behaviour as It’s possible to add or remove values in .libPaths(). You’ll almost certainly be adding values, there’s little reason to remove values.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html#where-packages-are-installed",
    "href": "chapters/pkg-management/persisting-libs.html#where-packages-are-installed",
    "title": "4  Persisting Packages",
    "section": "",
    "text": "Path\nDetails\n\n\n\n\n/local_disk0/.ephemeral_nfs/envs/rEnv-&lt;session-id&gt;\nThe first location is always a notebook specific directory, this is what allows each notebook session to have different libraries installed.\n\n\n/databricks/spark/R/lib\nOnly {SparkR} is found here\n\n\n/local_disk0/.ephemeral_nfs/cluster_libraries/r\nCluster libraries - you could also install packages here explicitly to share amongst all users (e.g. lib parameter of install.packages)\n\n\n/usr/local/lib/R/site-library\nPackages built into the Databricks Runtime\n\n\n/usr/lib/R/site-library\nEmpty\n\n\n/usr/lib/R/library\nBase R packages\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll following examples will use Unity Catalog Volumes. DBFS can be used but it’s not recommended.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html#persisting-a-package",
    "href": "chapters/pkg-management/persisting-libs.html#persisting-a-package",
    "title": "4  Persisting Packages",
    "section": "4.2 Persisting a Package",
    "text": "4.2 Persisting a Package\nThe recommended approach is to first install the library(s) you want to persist on a cluster via a notebook.\nFor example, let’s persist {leaflet} to a volume:\n\n1install.packages(\"leaflet\")\n\n# determine where the package was installed\n2pkg_location &lt;- find.package(\"leaflet\")\n\n# move package to volume\n3new_pkg_location &lt;- \"/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/my_packages\"\n4file.copy(from = pkg_location, to = new_pkg_location, recursive = TRUE)\n\n\n1\n\nInstalling {leaflet}\n\n2\n\nReturn path to package files, from what was explained before we know this will be a sub-directory of .libPaths() first path\n\n3\n\nDefine the path to volume where package will be persisted, make sure to adjust as needed\n\n4\n\nCopy the folder contents recursively to the volume\n\n\n\n\nAt this point the package is persisted, but if you restart the cluster or detach and reattach and try to load {leaflet} it will fail to load.\nThe last step is to adjust .libPaths() to include the volume path. You could make it the first value by:\n\n# adjust .libPaths\n1.libPaths(c(new_pkg_location, .libPaths()))\n\n\n1\n\nI recommend against making it the first value, will detail why in Ordering",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html#adjusting-.libpaths",
    "href": "chapters/pkg-management/persisting-libs.html#adjusting-.libpaths",
    "title": "4  Persisting Packages",
    "section": "4.3 Adjusting .libPaths()",
    "text": "4.3 Adjusting .libPaths()\n\n4.3.1 Ordering\nGiven that .libPaths() can return 6 values in a notebook you might wonder if there a “best” position to add your new volume path(s) to, that will depend on how you want packages to behave.\nA safe default is to add a path after the cluster libraries location (currently 3rd), this will make it appear as if the Databricks Runtime has been extended to include packages in the volume path(s).\nAlternatively you could add it after the first path and all users will still have the notebook scope package behaviour by default but cluster libraries may not load if they appear in the earlier paths under a different version.\nIt will be up to you to decide what works best.\n\n\n\n\n\n\nImportant\n\n\n\nI don’t recommend pre-pending .libPaths() with volume paths as packages will attempt to install to the first value and you cannot directly install packages to a volume path (due to volumes being backed onto cloud storage). This is why the example for persisting copies after installation.\n\n\nAn example of adjusting .libPaths() looks like:\n\nvolume_pkgs &lt;- \"/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/my_packages\"\n.libPaths(new = append(.libPaths(), volume_pkgs, after = 3))\n\n\n\n4.3.2 Helpful Functions\nThe examples can be used to build a set of functions to make this easier.\nCopying a Package\n\ncopy_package &lt;- function(name, destination) {\n  package_loc &lt;- find.package(name)\n  file.copy(from = package_loc, to = destination, recursive = TRUE)\n}\n\n# e.g. move {ggplot2} to volume\ncopy_package(\"ggplot2\", \"/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/my_packages\")\n\nAlter .libPaths()\n\nadd_lib_paths &lt;- function(path, after, version = FALSE) {\n1  if (version) {\n    rver &lt;- getRversion()\n    lib_path &lt;- file.path(path, rver)\n  } else {\n    lib_path &lt;- file.path(path)\n  }\n\n  # ensure directory exists\n  if (!file.exists(lib_path)) {\n    dir.create(lib_path, recursive = TRUE)\n  }\n\n  lib_path &lt;- normalizePath(lib_path, \"/\")\n\n  message(\"primary package path is now \", lib_path)\n  .libPaths(new = append(.libPaths(), lib_path, after = after))\n  lib_path\n}\n\n\n1\n\nAllows specifying version as TRUE or FALSE to suffix the supplied path with the current R version\n\n\n\n\n\n\n4.3.3 Avoiding Repetition\nTo avoid manually adjusting .libPaths() every notebook you can craft an init script or set environment variables, depending on the desired outcome.\n\n\n\n\n\n\nCaution\n\n\n\nIn practice this interferes with how Databricks sets up the environment, validate any changes thoroughly before rolling out to users.\n\n\n\n4.3.3.1 Init Script\n\n\n\n\n\n\nNote\n\n\n\nThis example appends to the existing Renviron.site file to ensure any settings defined as part of runtime are preserved.\nThe last two lines of the script are setting R_LIBS_SITE and R_LIBS_USER. Changing these lines can give you granular control over order for anything after the 1st value of .libPaths() as it’s injected when the notebook session starts.\n\n\n\n#!/bin/bash\n1volume_pkgs=/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/my_packages\ncat &lt;&lt;EOF &gt;&gt; \"/etc/R/Renviron.site\"\n2R_LIBS_USER=%U:/databricks/spark/R/lib:/local_disk0/.ephemeral_nfs/cluster_libraries/r:$volume_pkgs\nEOF\n\n\n1\n\nDefine the path(s) to add to R_LIBS_USER\n\n2\n\nAppend line to /etc/R/Renviron.site with location after cluster libraries, you can rearrange the paths as long as they remain : separated\n\n\n\n\n\n\n4.3.3.2 Environment Variables\n\n\n\n\n\n\nCaution\n\n\n\nHow the Databricks Runtime defines and uses the R environment variables is something that may change and should be tested carefully, especially if upgrading runtime versions.\n\n\nThere are particular environment variables (R_LIBS, R_LIBS_USER, R_LIBS_SITE) that can be set to initialise the library search path (.libPaths()).\nR_LIBS and R_LIBS_USER are defined as part of start-up processes in Databricks Runtime and they’ll be overridden, it’s easier to adjust via an Init Script.\nR_LIBS_SITE can be set via an environment variable but is referenced by /etc/R/Renviron.site and will provides limited control over where the path will appear in the .libPaths() order (it will appear 5th, after the packages included in the Databricks runtime) unless using an init script to alter /etc/R/Renviron.site directly.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html#organising-packages",
    "href": "chapters/pkg-management/persisting-libs.html#organising-packages",
    "title": "4  Persisting Packages",
    "section": "4.4 Organising Packages",
    "text": "4.4 Organising Packages\nWhen going down this route of persisting packages you should consider how this is organised and managed long term to avoid making things messy.\nSome practices you can consider include:\n\nMaintaining directories of packages per project, team, or user\nEnsuring directories are specific to an R version (and potentially even Databricks Runtime version)\nCoupling the use of persistence with {renv}",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/renv.html",
    "href": "chapters/pkg-management/renv.html",
    "title": "5  {renv}",
    "section": "",
    "text": "Under Development",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>`{renv}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/odbc-dbplyr.html",
    "href": "chapters/data-eng/odbc-dbplyr.html",
    "title": "6  {dbplyr} and {odbc}",
    "section": "",
    "text": "Under Development",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>`{dbplyr}` and `{odbc}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/sparkr-migration.html",
    "href": "chapters/data-eng/sparkr-migration.html",
    "title": "7  {SparkR} to {sparklyr}",
    "section": "",
    "text": "7.1 Introduction\nBeginning with Spark 4.x, {SparkR} will be deprecated. Going forward, {sparklyr} will be the recommended R package for working with Apache Spark. This guide is intended to help users understand the differences between {SparkR} and {sparklyr} across Spark APIs, and aid in code migration from one to the other. It combines basic concepts with specific function mappings where appropriate.",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{SparkR}` to `{sparklyr}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/sparkr-migration.html#introduction",
    "href": "chapters/data-eng/sparkr-migration.html#introduction",
    "title": "7  {SparkR} to {sparklyr}",
    "section": "",
    "text": "7.1.1 Overview of {SparkR} and {sparklyr}\n{SparkR} and {sparklyr} are both R packages designed to work with Apache Spark, but differ significantly in design, syntax, and integration with the broader R ecosystem.\n{SparkR} is developed as part of Apache Spark itself, and its design mirrors Spark’s core APIs. This makes it straightforward for those familiar with Spark’s other language interfaces - Scala and Python. However, this may be less intuitive for R users accustomed to the tidyverse.\nIn contrast, {sparklyr} is developed and maintained by Posit PBC with a focus on providing a more R-friendly experience. It leverages {dplyr} syntax, which is highly familiar to users of the {tidyverse}, enabling them to interact with Spark DataFrames using R-native verbs like select(), filter(), and mutate(). This makes {sparklyr} easier to learn for R users, especially those who are not familiar with Spark’s native API.",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{SparkR}` to `{sparklyr}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/sparkr-migration.html#environment-setup",
    "href": "chapters/data-eng/sparkr-migration.html#environment-setup",
    "title": "7  {SparkR} to {sparklyr}",
    "section": "7.2 Environment setup",
    "text": "7.2 Environment setup\n\n7.2.1 Installation\nIf working inside of the Databricks Workspace, no installation is required - you can simply load {sparklyr} with library(sparklyr). To install {sparklyr} on a machine outside of Databricks, follow these steps.\n\n\n7.2.2 Connecting to Spark\nWhen working inside of the Databricks workspace, you can connect to Spark with {sparklyr} with the following code:\n\nlibrary(sparklyr)\nsc &lt;- spark_connect(method = \"databricks\")\n\nWhen connecting to Databricks remotely via Databricks Connect, a slightly different method is used:\n\nsc &lt;- spark_connect(method = \"databricks_connect\")\n\nFor more details and an extended tutorial on Databricks Connect with {sparklyr}, see the official documentation.",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{SparkR}` to `{sparklyr}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/sparkr-migration.html#reading-writing-data",
    "href": "chapters/data-eng/sparkr-migration.html#reading-writing-data",
    "title": "7  {SparkR} to {sparklyr}",
    "section": "7.3 Reading & Writing Data",
    "text": "7.3 Reading & Writing Data\nIn contrast to generic read.df() and write.df() functions in {SparkR}, {sparklyr} has a family of spark_read_*()and spark_write_*() functions to load and save data. There are also unique functions to create Spark DataFrames or Spark SQL temporary views from R data frames in memory.\n\n7.3.1 TL;DR\n\nRecommended function mapping\n\n\n{SparkR}\n{sparklyr}\n\n\n\n\ncreateDataFrame()\ncopy_to()\n\n\ncreateOrReplaceTempView()\nUse invoke() with method directly\n\n\nsaveAsTable()\nspark_write_table()\n\n\nwrite.df()\nspark_write_&lt;format&gt;()\n\n\ntableToDF()\ntbl() (or spark_read_table() when it’s fixed)\n\n\nread.df()\nspark_read_&lt;format&gt;()\n\n\n\n\n\n7.3.2 Loading Data\nTo convert a R data frame to a Spark DataFrame, or to create a temporary view out of a DataFrame to apply SQL to it:\nSparkR\n\n# create SparkDataFrame from R data frame\nmtcars_df &lt;- createDataFrame(mtcars)\n\nsparklyr\n\n# create SparkDataFrame and name temporary view 'mtcars_tmp'\nmtcars_tbl &lt;- copy_to(\n  sc,\n  df = mtcars,\n1  name = \"mtcars_tmp\",\n  overwrite = TRUE,\n2  memory = FALSE\n) \n\n\n1\n\ncopy_to() will create a temporary view of the data with the given name, you can use name to reference data if using SQL directly (e.g. sdf_sql()).\n\n2\n\nDefault behaviour of copy_to() will set memory as TRUE which caches the table. This helps when reading the data multiple times - sometimes its worth setting to FALSE if data is read as one-off.\n\n\n\n\n\n\n7.3.3 Creating Views\nSparkR\n\n# create temporary view\ncreateOrReplaceTempView(mtcars_df, \"mtcars_tmp_view\")\n\nsparklyr\n\n# direct equivlent from SparkR requires `invoke`\n# usually redundant given `copy_to` already creates a temp view\nspark_dataframe(mtcars_tbl) |&gt;\n  invoke(\"createOrReplaceTempView\", \"mtcars_tmp_view\")\n\n\n\n7.3.4 Writing Data\nSparkR\n\n# save SparkDataFrame to Unity Catalog\nsaveAsTable(\n  mtcars_df,\n  tableName = \"&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;\",\n  mode = \"overwrite\"\n)\n\n# save DataFrame using delta format to local filesystem\nwrite.df(\n  mtcars_df,\n  path = \"file:/&lt;path/to/save/delta/mtcars&gt;\",\n1  source = \"delta\",\n  mode = \"overwrite\"\n)\n\n\n1\n\nwrite.df() supports other formats via source parameter\n\n\n\n\nsparklyr\n\n# save tbl_spark to Unity Catalog\nspark_write_table(\n  mtcars_tbl,\n  name = \"&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;\",\n  mode = \"overwrite\"\n)\n\n# save tbl_spark using delta format to local filesystem\nspark_write_delta(\n  mtcars_tbl,\n  path = \"file:/&lt;path/to/save/delta/mtcars&gt;\",\n  mode = \"overwrite\"\n)\n\n# Using {DBI}\nlibrary(DBI)\ndbWriteTable(\n  sc,\n  value = mtcars_tbl,\n  name = \"&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;\",\n  overwrite = TRUE\n)\n\n\n\n7.3.5 Reading Data\nSparkR\n\n# load Unity Catalog table as SparkDataFrame\ntableToDF(\"&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;\")\n\n# load csv file into SparkDataFrame\nread.df(\n  path = \"file:/&lt;path/to/read/csv/data.csv&gt;\",\n  source = \"csv\",\n  header = TRUE,\n  inferSchema = TRUE\n)\n\n# load delta from local filesystem as SparkDataFrame\nread.df(\n  path = \"file:/&lt;path/to/read/delta/mtcars&gt;\",\n  source = \"delta\"\n)\n\n# load data from a table using SQL\n# recommended to use `tableToDF`\nsql(\"SELECT * FROM &lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;\")\n\nsparklyr\n\n# currently has an issue if using Unity Catalog\n# recommend using `tbl` (example below)\nspark_read_table(sc, \"&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;\", memory = FALSE)\n\n# load table from Unity Catalog with {dplyr}\ntbl(sc, \"&lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;\")\n\n# or using `in_catalog`\ntbl(sc, in_catalog(\"&lt;catalog&gt;\", \"&lt;schema&gt;\", \"&lt;table&gt;\"))\n\n# load csv from local filesystem as tbl_spark\nspark_read_csv(\n  sc,\n  name = \"mtcars_csv\",\n  path = \"file:/&lt;path/to/delta/mtcars&gt;\",\n  header = TRUE,\n  infer_schema = TRUE\n)\n\n# load delta from local filesystem as tbl_spark\nspark_read_delta(\n  sc,\n  name = \"mtcars_delta\",\n  path = \"file:/tmp/test/sparklyr1\"\n)\n\n# using SQL\nsdf_sql(sc, \"SELECT * FROM &lt;catalog&gt;.&lt;schema&gt;.&lt;table&gt;\")",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{SparkR}` to `{sparklyr}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/sparkr-migration.html#processing-data",
    "href": "chapters/data-eng/sparkr-migration.html#processing-data",
    "title": "7  {SparkR} to {sparklyr}",
    "section": "7.4 Processing Data",
    "text": "7.4 Processing Data\n\n7.4.1 Select, Filter\nSparkR\n\n# select specific columns\nselect(mtcars_df, \"mpg\", \"cyl\", \"hp\")\n\n# filter rows where mpg &gt; 20\nfilter(mtcars_df, mtcars_df$mpg &gt; 20)\n\nsparklyr\n\n# select specific columns\nmtcars_tbl |&gt;\n  select(mpg, cyl, hp)\n\n# filter rows where mpg &gt; 20\nmtcars_tbl |&gt;\n  filter(mpg &gt; 20)\n\n\n\n7.4.2 Adding Columns\nSparkR\n\n# add a new column 'power_to_weight' (hp divided by wt)\nwithColumn(mtcars_df, \"power_to_weight\", mtcars_df$hp / mtcars_df$wt)\n\nsparklyr\n\n# add a new column 'power_to_weight' (hp divided by wt)\nmtcars_tbl |&gt;\n  mutate(power_to_weight = hp / wt)\n\n\n\n7.4.3 Grouping & Aggregation\nSparkR\n\n# calculate average mpg and hp by number of cylinders\nmtcars_df |&gt;\n  groupBy(\"cyl\") |&gt;\n  summarize(\n    avg_mpg = avg(mtcars_df$mpg),\n    avg_hp = avg(mtcars_df$hp)\n  )\n\nsparklyr\n\n# calculate average mpg and hp by number of cylinders\nmtcars_tbl |&gt;\n  group_by(cyl) |&gt;\n  summarize(\n    avg_mpg = mean(mpg),\n    avg_hp = mean(hp)\n  )\n\n\n\n7.4.4 Joins\nSuppose we have another dataset with cylinder labels that we want to join to mtcars.\nSparkR\n\n# create another SparkDataFrame with cylinder labels\ncylinders &lt;- data.frame(\n  cyl = c(4, 6, 8),\n  cyl_label = c(\"Four\", \"Six\", \"Eight\")\n)\ncylinders_df &lt;- createDataFrame(cylinders)\n\n# join mtcars_df with cylinders_df\njoin(\n  x = mtcars_df,\n  y = cylinders_df,\n  mtcars_df$cyl == cylinders_df$cyl,\n  joinType = \"inner\"\n)\n\nsparklyr\n\n# create another SparkDataFrame with cylinder labels\ncylinders &lt;- data.frame(\n  cyl = c(4, 6, 8),\n  cyl_label = c(\"Four\", \"Six\", \"Eight\")\n)\ncylinders_tbl &lt;- copy_to(sc, cylinders, \"cylinders\", overwrite = TRUE)\n\n# join mtcars_tbl with cylinders_tbl\nmtcars_tbl |&gt;\n  inner_join(cylinders_tbl, by = join_by(cyl))",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{SparkR}` to `{sparklyr}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/sparkr-migration.html#user-defined-functions-udfs",
    "href": "chapters/data-eng/sparkr-migration.html#user-defined-functions-udfs",
    "title": "7  {SparkR} to {sparklyr}",
    "section": "7.5 User Defined Functions (UDFs)",
    "text": "7.5 User Defined Functions (UDFs)\nSuppose we want to categorize horsepower into ‘High’ or ‘Low’ based on a threshold\n\n\n\n\n\n\nNote\n\n\n\nThis is an arbitrary example; in practice we would recommend case_when() combined with mutate().\n\n\n\n# define custom function\ncategorize_hp &lt;- function(df) {\n  df$hp_category &lt;- ifelse(df$hp &gt; 150, \"High\", \"Low\")\n  df\n}\n\nSparkR\nUDFs in {SparkR} require an output schema, which we define first.\n\n# define the schema for the output DataFrame\nschema &lt;- structType(\n  structField(\"mpg\", \"double\"),\n  structField(\"cyl\", \"double\"),\n  structField(\"disp\", \"double\"),\n  structField(\"hp\", \"double\"),\n  structField(\"drat\", \"double\"),\n  structField(\"wt\", \"double\"),\n  structField(\"qsec\", \"double\"),\n  structField(\"vs\", \"double\"),\n  structField(\"am\", \"double\"),\n  structField(\"gear\", \"double\"),\n  structField(\"carb\", \"double\"),\n  structField(\"hp_category\", \"string\")\n)\n\nTo apply this function to each partition of a Spark DataFrame, we use dapply().\n\n# apply function across partitions using dapply\ndapply(\n  mtcars_df,\n  categorize_hp,\n  schema\n)\n\nTo apply the same function to each group of a Spark DataFrame, we use gapply(). Note that the schema is still required.\n\n# apply function across groups\ngapply(\n  mtcars_df,\n  cols = \"hp\",\n  func = categorize_hp,\n  schema = schema\n)\n\nsparklyr\n\n\n\n\n\n\nTip\n\n\n\nHighly recommended ‘Distrubuting R Computations’ guide in {sparklyr} docs, it goes into much more detail on spark_apply().\n\n\n\n\n\n\n\n\nNote\n\n\n\nspark_apply() will do it’s best to derive the column names and schema of the output via sampling 10 rows, this can add overhead that can be omitted by specifying the columns parameter.\n\n\n\n# ensure that {arrow} is loaded, otherwise may encounter cryptic errors\nlibrary(arrow)\n\n# apply the function over data \n# by default applies to each partition\nmtcars_tbl |&gt;\n  spark_apply(f = categorize_hp)\n\n# apply the function over data \n# Using `group_by` to apply data over groups\nmtcars_tbl |&gt;\n  spark_apply(\n    f = summary,\n1    group_by = \"hp\"\n  )\n\n\n1\n\nIn this example group_by isn’t changing the resulting output as the functions behaviour is applied to rows independently. Other functions that operate on a set of rows would behave differently (e.g. summary()).\n\n\n\n\nSparkR::spark.lapply() is unique in that it applies to lists in R, as opposed to DataFrames. There is no exact equivalent in {sparklyr}, but using spark_apply() with a DataFrame with unique IDs and grouping it by ID will behave similarly in many cases, or, more creative functions that operate on a row-wise basis.\nSparkR\n\n# define a list of integers\nnumbers &lt;- list(1, 2, 3, 4, 5)\n\n# define a function to apply\nsquare &lt;- function(x) {\n  x * x\n}\n\n# apply the function over list using spark\nspark.lapply(numbers, square)\n\nsparklyr\n\n# create a spark DataFrame of given length\nsdf &lt;- sdf_len(sc, 5, repartition = 1)\n\n# apply function to each partition of data.frame\n1spark_apply(sdf, f = nrow)\n\n# apply function to each row (option 1)\n2spark_apply(sdf, f = nrow, group_by = \"id\")\n\n# apply function to each row (option 2)\n3row_func &lt;- function(df) {\n  df |&gt;\n    dplyr::rowwise() |&gt;\n    dplyr::mutate(x = id * 2)\n}\nspark_apply(sdf, f = row_func)\n\n\n1\n\nspark_apply() defaults to processing data based on number of partitions, in this case it will return a single row as due to repartition = 1.\n\n2\n\nTo force behaviour like spark.lapply() you can create a DataFrame with N rows and force grouping with group_by set to a unique row identifier (in this case it’s the id column automatically generated by sdf_len()). This will return N rows.\n\n3\n\nThis requires writing a function that operates across rows of a data.frame, in some occassions this may be faster relative to (2). Specifying group_by in optional for this example. This example does not require rowwise(), but is just to illustrate one method to force computations to be for every row. Your function should take care to import required packages, etc.",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{SparkR}` to `{sparklyr}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/sparkr-migration.html#machine-learning",
    "href": "chapters/data-eng/sparkr-migration.html#machine-learning",
    "title": "7  {SparkR} to {sparklyr}",
    "section": "7.6 Machine learning",
    "text": "7.6 Machine learning\nFull examples for each package can be found in the official reference for {SparkR} and {sparklyr}, respectively.\nIf not using Spark MLlib it is recommended to use UDFs to train with the library of your choice (e.g. {xgboost}).\n\n7.6.1 Linear regression\nSparkR\n\n# select features\ntraining_df &lt;- select(mtcars_df, \"mpg\", \"hp\", \"wt\")\n\n# fit the model using Generalized Linear Model (GLM)\nlinear_model &lt;- spark.glm(training_df, mpg ~ hp + wt, family = \"gaussian\")\n\n# view model summary\nsummary(linear_model)\n\nsparklyr\n\n# select features\ntraining_tbl &lt;- mtcars_tbl |&gt;\n  select(mpg, hp, wt)\n\n# fit the model using Generalized Linear Model\nlinear_model &lt;- training_tbl |&gt;\n  ml_linear_regression(response = \"mpg\", features = c(\"hp\", \"wt\"))\n\n# view model summary\nsummary(linear_model)\n\n\n\n7.6.2 K-means clustering\nSparkR\n\n# apply KMeans clustering with 3 clusters using mpg and hp as features\nkmeans_model &lt;- spark.kmeans(mtcars_df, mpg ~ hp, k = 3)\n\n# get cluster predictions\n1predict(kmeans_model, mtcars_df)\n\n\n1\n\nPredicting on input data to keep example simple\n\n\n\n\nsparklyr\n\n# use mpg and hp as features\nfeatures_tbl &lt;- mtcars_tbl |&gt;\n  select(mpg, hp)\n\n# assemble features into a vector column\nfeatures_vector_tbl &lt;- features_tbl |&gt;\n  ft_vector_assembler(\n    input_cols = c(\"mpg\", \"hp\"),\n    output_col = \"features\"\n  )\n\n# apply K-Means clustering\nkmeans_model &lt;- features_vector_tbl |&gt;\n  ml_kmeans(features_col = \"features\", k = 3)\n\n# get cluster predictions\n1ml_predict(kmeans_model, features_vector_tbl)\n\n\n1\n\nPredicting on input data to keep example simple",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{SparkR}` to `{sparklyr}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/sparkr-migration.html#performance-and-optimization",
    "href": "chapters/data-eng/sparkr-migration.html#performance-and-optimization",
    "title": "7  {SparkR} to {sparklyr}",
    "section": "7.7 Performance and optimization",
    "text": "7.7 Performance and optimization\n\n7.7.1 Collecting\nBoth {SparkR} and {sparklyr} use the same function name, collect(), to convert Spark DataFrames to R data frames. In general, only collect small amounts of data back to R data frames or the Spark driver will run out of memory, crashing your script (and you want to use Spark to accelerate workloads as much as possible!).\nTo prevent out of memory errors, {SparkR} has built-in optimizations in Databricks Runtime that help collect data or execute user-defined functions (which also require collecting data to workers). To ensure smooth performance with {sparklyr} for collecting data and UDFs, make sure to load the {arrow} package in your scripts.\n\n# when on Databricks DBR 14.3 or higher {arrow} is pre-installed\nlibrary(arrow)\n\nIf you encounter issues with collecting large datasets with {sparklyr} the methods documented here may assist, however, hitting this is typically an indicator that you should defer more work to Spark.\n\n\n7.7.2 In-Memory Partitioning\nSparkR\n\n# repartition the SparkDataFrame based on 'cyl' column\nrepartition(mtcars_df, col = mtcars_df$cyl)\n\n# repartition the SparkDataFrame to number of partitions\nrepartition(mtcars_df, numPartitions = 10)\n\n# coalesce the SparkDataFrame to number of partitions\ncoalesce(mtcars_df, numPartitions = 1)\n\n# get number of partitions\ngetNumPartitions(mtcars_df)\n\nsparklyr\n\n# repartition the tbl_spark based on 'cyl' column\nsdf_repartition(mtcars_tbl, partition_by = \"cyl\")\n\n# repartition the tbl_spark to number of partitions\nsdf_repartition(mtcars_tbl, partitions = 10)\n\n# coalesce the tbl_spark to number of partitions\nsdf_coalesce(mtcars_tbl, partitions = 1)\n\n# get number of partitions\nsdf_num_partitions(mtcars_tbl)\n\n\n\n7.7.3 Caching\nSparkR\n\n# cache the SparkDataFrame in memory\ncache(mtcars_df)\n\nsparklyr\n\n# cache the tbl_spark in memory\ntbl_cache(sc, name = \"mtcars_tmp\")",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{SparkR}` to `{sparklyr}`</span>"
    ]
  },
  {
    "objectID": "chapters/misc/htmlwidgets.html",
    "href": "chapters/misc/htmlwidgets.html",
    "title": "8  {htmlwidgets} in notebooks",
    "section": "",
    "text": "When you try to view a {htmlwidget} based visualisation (e.g. {leaflet}) in a Databricks notebook you’ll find there is no rendered output by default.\nThe Databricks documentation details how to get this working but requires specification of the workspace URL explicitly and writes out files to DBFS’s FileStore without cleaning up after itself.\nThe new method avoids those steps and is drastically simplified and easier to use, just run the below function in a Databricks notebook:\n\nenable_htmlwidgets &lt;- function(height = 450) {\n\n  # new option to control default widget height, default is 450px\n1  options(db_htmlwidget_height = height)\n\n2  system(\"apt-get update && apt-get --yes install pandoc\", intern = T)\n3  if (!base::require(\"htmlwidgets\")) {\n    utils::install.packages(\"htmlwidgets\")\n  }\n\n  # new method will fetch height based on new option, or default to 450px\n4  new_method &lt;- function(x, ...) {\n    x$height &lt;- getOption(\"db_htmlwidget_height\", 450)\n    file &lt;- tempfile(fileext = \".html\")\n    htmlwidgets::saveWidget(x, file = file)\n    contents &lt;- as.character(rvest::read_html(file))\n    displayHTML(contents)\n  }\n\n5  utils::assignInNamespace(\"print.htmlwidget\", new_method, ns = \"htmlwidgets\")\n  invisible(list(default_height = height, print = new_method))\n  \n}\n\n\n1\n\nThe height of the htmlwidget output is controlled via an option (db_htmlwidget_height), this allows the height to be adjusted without re-running the function\n\n2\n\nInstalling pandoc as it’s required to use htmlwidgets::saveWidget\n\n3\n\nEnsure that {htmlwidgets} is installed\n\n4\n\nFunction that writes the widget to a temporary file as a self-contained html file and then reads the contents and presents via displayHTML\n\n5\n\nOverride the htmlwidgets::print.htmlwidget method",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>`{htmlwidgets}` in notebooks</span>"
    ]
  },
  {
    "objectID": "chapters/misc/odbc-oauth.html",
    "href": "chapters/misc/odbc-oauth.html",
    "title": "9  OAuth 🤝 {odbc}",
    "section": "",
    "text": "9.1 U2M Example\nWhen using {odbc} to connect to Databricks clusters and SQL warehouses you’ll likely have used a personal access token (PAT). It’s not uncommon for workspace administrators to disable the use of PATs.\nIf you are unable to create a PAT you are still able to connect to Databricks but you’ll need to use OAuth (either M2M or U2M).\nUser-to-machine (U2M) is typically what you’d want to use. Good news, the Databricks ODBC driver supports both since 2.7.5.\nWhen running this code you should be prompted to login to the workspace or you’ll see a window that says “success”. You can close the window and continue working in R.\nlibrary(odbc)\nlibrary(DBI)\n\ncon &lt;- DBI::dbConnect(\n1  drv = odbc::databricks(),\n2  httpPath = \"/sql/1.0/warehouses/&lt;warehouse-id&gt;\",\n3  workspace = \"&lt;workspace-name&gt;.cloud.databricks.com\",\n4  authMech = 11,\n  auth_flow = 2\n)\n\n\n1\n\n{odbc} recently added odbc::databricks() to simplify connecting to Databricks (requires version &gt;=1.4.0)\n\n2\n\nThe httpPath can be found in the ‘Connection Details’ tab of a SQL warehouse\n\n3\n\nworkspace refers to the workspace URL, also found in ‘Connection Details’ tab as ‘Server hostname’\n\n4\n\nThe docs mention setting AuthMech to 11 and Auth_Flow to 2",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>OAuth 🤝 `{odbc}`</span>"
    ]
  },
  {
    "objectID": "chapters/misc/odbc-oauth.html#u2m-example",
    "href": "chapters/misc/odbc-oauth.html#u2m-example",
    "title": "9  OAuth 🤝 {odbc}",
    "section": "",
    "text": "Note\n\n\n\nOAuth U2M or OAuth 2.0 browser-based authentication works only with applications that run locally. It does not work with server-based or cloud-based applications.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>OAuth 🤝 `{odbc}`</span>"
    ]
  }
]