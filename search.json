[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R on Databricks Compendium",
    "section": "",
    "text": "What is this?\n\n\n\n\n\n\nUnder Development\n\n\n\n\n\n\n\n\n\n\n\n\nThis is not intended to be an exhaustive guide, it’s currently a place for me to document and collate useful information regarding R on Databricks.\n\n\n\n\n\n\nThere aren’t many definitive examples of how to use R and Databricks together - hopefully the content here will serve as a useful resource.",
    "crumbs": [
      "What is this?"
    ]
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html",
    "href": "chapters/mlflow/log-to-uc.html",
    "title": "1  Log R Models to Unity Catalog",
    "section": "",
    "text": "1.1 Unity Catalog Model Requirements\nFor models to be logged into Unity Catalog they must have a model signature. The Model signature defines the schema for model inputs/outputs.\nTypically when using python this would be inferred via model input examples. Input examples are optional but strongly recommended.\nThe documentation discusses signature enforcement, currently this isn’t implemented for R. Therefore you can decide if the signature is a dummy value for the sake of moving forward, or correct to clearly communicate the behaviour of the model.\nSo, let’s look at the existing code to log models in the crate flavour:\nmlflow_save_model.crate &lt;- function(model, path, model_spec=list(), ...) {\n1  if (dir.exists(path)) unlink(path, recursive = TRUE)\n  dir.create(path)\n\n2  serialized &lt;- serialize(model, NULL)\n\n3  saveRDS(\n    serialized,\n    file.path(path, \"crate.bin\")\n  )\n\n4  model_spec$flavors &lt;- append(model_spec$flavors, list(\n    crate = list(\n      version = \"0.1.0\",\n      model = \"crate.bin\"\n    )\n  ))\n  mlflow_write_model_spec(path, model_spec)\n  model_spec\n}\n\n\n1\n\nCreate the directory to save the model if it doesn’t exist, if it does, empty it\n\n2\n\nSerialise the model, which is an object of class crate (from {carrier} package)\n\n3\n\nSave the serialised model via saveRDS to the directory as crate.bin\n\n4\n\nDefine the model specification, this contains metadata required ensure reproducibility. In this case it’s only specifying a version and what file the model can be found within.\nThe missing puzzle piece is the definition of a signature. Instead of explicitly adding code to the crate flavour itself, we’ll take advantage of the model_spec parameter.\nThat means we can focus on mlflow::mlflow_log_model directly, we’d need to adjust the code as follows:\n1mlflow_log_model &lt;- function(model, artifact_path, ...) {\n  \n  temp_path &lt;- fs::path_temp(artifact_path)\n  \n  model_spec &lt;- mlflow_save_model(\n    model, path = temp_path,\n2    model_spec = list(\n      utc_time_created = mlflow_timestamp(),\n      run_id = mlflow_get_active_run_id_or_start_run(),\n      artifact_path = artifact_path,\n      flavors = list()\n    ),\n  ...)\n  \n  res &lt;- mlflow_log_artifact(path = temp_path, artifact_path = artifact_path)\n  \n  tryCatch({\n    mlflow:::mlflow_record_logged_model(model_spec)\n  },\n  error = function(e) {\n    warning(\n      paste(\"Logging model metadata to the tracking server has failed, possibly due to older\",\n            \"server version. The model artifacts have been logged successfully.\",\n            \"In addition to exporting model artifacts, MLflow clients 1.7.0 and above\",\n            \"attempt to record model metadata to the  tracking store. If logging to a\",\n            \"mlflow server via REST, consider  upgrading the server version to MLflow\",\n            \"1.7.0 or above.\", sep=\" \")\n    )\n  })\n  res\n}\n\n\n1\n\nAdd a new parameter signature\n\n2\n\nPropagate signature to the model_spec parameter when invoking mlflow::mlflow_save_model\nBenefit of this method is that all model flavors will inherit the capability to log a signature.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Log R Models to Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#unity-catalog-model-requirements",
    "href": "chapters/mlflow/log-to-uc.html#unity-catalog-model-requirements",
    "title": "1  Log R Models to Unity Catalog",
    "section": "",
    "text": "Important\n\n\n\nIt’s important to clarify that for python the signature is enforced at time of inference not when registering the model to Unity Catalog.\nThe signature correctness is not validated when registering the model, it just has to be syntactically valid.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Log R Models to Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#working-through-the-solution",
    "href": "chapters/mlflow/log-to-uc.html#working-through-the-solution",
    "title": "1  Log R Models to Unity Catalog",
    "section": "1.2 Working Through the Solution",
    "text": "1.2 Working Through the Solution\nTo keep things simple we’ll be logging a “model” (a function which divides by two).\n\nhalf &lt;- function(x) x / 2\n\nhalf(1:10)\n\n [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nWithout any changes, a simplified example of logging to {mlflow} would look like:\n\nlibrary(carrier)\nlibrary(mlflow)\n\nwith(mlflow_start_run(), {\n  # typically you'd do more modelling related activities here\n  model &lt;- carrier::crate(~half(.x))\n1  mlflow_log_model(model, \"model\")\n})\n\n\n1\n\nAs discussed earlier, this is where things start to go awry with respect to Unity Catalog\n\n\n\n\n\n1.2.1 Patching mlflow_log_model\n\n\n\n\n\n\nNote\n\n\n\nTechnically, patching mlflow_log_model isn’t the only way to achieve this fix - you could modify the yaml after it’s written.\nI won’t be showing that method as It’s just as tedious and can change depending on the model flavour (with respect to where artifacts may reside), patching is more robust.\n\n\n\n1mlflow_log_model &lt;- function(model, artifact_path, signature = NULL, ...) {\n  \n2  format_signature &lt;- function(signature) {\n    lapply(signature, function(x) {\n      jsonlite::toJSON(x, auto_unbox = TRUE)\n    })\n  }\n  \n  temp_path &lt;- fs::path_temp(artifact_path)\n  \n  model_spec &lt;- mlflow_save_model(model, path = temp_path, model_spec = list(\n    utc_time_created = mlflow:::mlflow_timestamp(),\n    run_id = mlflow:::mlflow_get_active_run_id_or_start_run(),\n    artifact_path = artifact_path, \n    flavors = list(),\n3    signature = format_signature(signature)\n  ), ...)\n  \n  res &lt;- mlflow_log_artifact(path = temp_path, artifact_path = artifact_path)\n  \n  tryCatch({\n    mlflow:::mlflow_record_logged_model(model_spec)\n  },\n  error = function(e) {\n    warning(\n      paste(\"Logging model metadata to the tracking server has failed, possibly due to older\",\n            \"server version. The model artifacts have been logged successfully.\",\n            \"In addition to exporting model artifacts, MLflow clients 1.7.0 and above\",\n            \"attempt to record model metadata to the  tracking store. If logging to a\",\n            \"mlflow server via REST, consider  upgrading the server version to MLflow\",\n            \"1.7.0 or above.\", sep=\" \")\n    )\n  })\n  res\n}\n\n# overriding the function in the existing mlflow namespace \nassignInNamespace(\"mlflow_log_model\", mlflow_log_model, ns = \"mlflow\")\n\n\n1\n\nsignature has been added to function parameters, it’s defaulting to NULL so that existing code won’t break\n\n2\n\nAdding format_signature function so don’t need to write JSON by hand, adding this within function for simplicity\n\n3\n\nsignature is propagated to mlflow_save_model’s model_spec parameter which will write a valid signature\n\n\n\n\n\n\n1.2.2 Logging Model with a Signature\n\nwith(mlflow_start_run(), {\n  # typically you'd do more modelling related activities here\n  model &lt;- carrier::crate(~half(.x))\n1  signature &lt;- list(\n    inputs = list(list(type = \"double\", name = \"x\")),\n    outputs = list(list(type = \"double\"))\n  )\n2  mlflow_log_model(model, \"model\", signature = signature)\n})\n\n\n1\n\nExplicitly defining a signature, a list that contains input and outputs, each are lists of lists respectively\n\n2\n\nPassing defined signature to the now patched mlflow_log_model function\n\n\n\n\n\n\n1.2.3 Registration to Unity Catalog\nNow that the prerequisite of adding a model signature has been satisfied there is one last hurdle to overcome, registering to Unity Catalog.\nThe hurdle is due to {mlflow} not having been updated yet to support registration to Unity Catalog directly. The easiest way to overcome this is to simply register the run via python.\nFor example:\n\nimport mlflow\nmlflow.set_registry_uri(\"databricks-uc\")\n\ncatalog = \"main\"\nschema = \"default\"\nmodel_name = \"my_model\"\n1run_uri = \"runs:/&lt;run_id&gt;/model\"\n\nmlflow.register_model(run_uri, f\"{catalog}.{schema}.{model_name}\")\n\n\n1\n\nYou’ll need to either get the run_uri programmatically or copy it manually\n\n\n\n\nTo do this with R you’ll need to make a series of requests to Unity Catalog endpoints for registering model, the specific steps are:\n\n(Optional) Create a new model in Unity Catalog\n\nPOST request on /api/2.0/mlflow/unity-catalog/registered-models/create\n\nname: 3 tiered namespace (e.g. main.default.my_model)\n\n\nCreate a version for the model\n\nPOST request on /api/2.0/mlflow/unity-catalog/model-versions/create\n\nname: 3 tiered namespace (e.g. main.default.my_model)\nsource: URI indicating the location of the model artifacts\nrun_id: run_id from tracking server that generated the model\nrun_tracking_server_id: Workspace ID of run that generated the model\n\nThis will return storage_location and version\n\nCopy model artifacts\n\nNeed to copy the artifacts to storage_location from step (2)\n\nFinalise the model version\n\nPOST request on /api/2.0/mlflow/unity-catalog/model-versions/finalize\n\nname: 3 tiered namespace (e.g. main.default.my_model)\nversion: version returned from step (2)\n\n\n\nIt’s considerably easier to just use Python to register the model at this time.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Log R Models to Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/log-to-uc.html#fixing-mlflow",
    "href": "chapters/mlflow/log-to-uc.html#fixing-mlflow",
    "title": "1  Log R Models to Unity Catalog",
    "section": "1.3 Fixing mlflow",
    "text": "1.3 Fixing mlflow\nIdeally this page wouldn’t exist and {mlflow} would support Unity Catalog. Hopefully sometime soon I find the time to make a pull request myself - until then this serves as a guide.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Log R Models to Unity Catalog</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/model-serving.html",
    "href": "chapters/mlflow/model-serving.html",
    "title": "2  Model Serving",
    "section": "",
    "text": "2.1 Serving Overview",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Serving</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#serving-overview",
    "href": "chapters/mlflow/model-serving.html#serving-overview",
    "title": "2  Model Serving",
    "section": "",
    "text": "Caution\n\n\n\nWork In Progress",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Serving</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#xgboost",
    "href": "chapters/mlflow/model-serving.html#xgboost",
    "title": "2  Model Serving",
    "section": "2.2 XGBoost",
    "text": "2.2 XGBoost\n\n\n\n\n\n\nCaution\n\n\n\nWork In Progress\n\n\nXGBoost is relatively straightforward since it the artifacts generated when saving the model are universally understood across each language the library is available in.",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Serving</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#onnx",
    "href": "chapters/mlflow/model-serving.html#onnx",
    "title": "2  Model Serving",
    "section": "2.3 ONNX",
    "text": "2.3 ONNX\n\n\n\n\n\n\nCaution\n\n\n\nWork In Progress",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Serving</span>"
    ]
  },
  {
    "objectID": "chapters/mlflow/model-serving.html#crate",
    "href": "chapters/mlflow/model-serving.html#crate",
    "title": "2  Model Serving",
    "section": "2.4 Crate",
    "text": "2.4 Crate\n\n\n\n\n\n\nCaution\n\n\n\nWork In Progress",
    "crumbs": [
      "Mlflow",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Model Serving</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html",
    "href": "chapters/pkg-management/fast-installs.html",
    "title": "3  Faster Package Installs",
    "section": "",
    "text": "3.1 Setting Repo within Notebook\nThe quickest method is to follow the wizard and adjust the repos option:\n# set the user agent string otherwise pre-compiled binarys aren't used\n# e.g. selecting Ubuntu 22.04 in wizard \noptions(\n1  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])),\n  repos = \"https://packagemanager.posit.co/cran/__linux__/jammy/latest\"\n)\n\n\n1\n\nHTTPUserAgent is required when using R 3.6 or later\nThis works well but not all versions of the Databricks Runtime use the same version of Ubuntu.\nIt’s easier to detect the Ubuntu release code name dynamically:\n1release &lt;- system(\"lsb_release -c --short\", intern = T)\n\n# set the user agent string otherwise pre-compiled binarys aren't used\noptions(\n  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"])),\n  repos = paste0(\"https://packagemanager.posit.co/cran/__linux__/\", release, \"/latest\")\n)\n\n\n1\n\nsystem is used to run the command to retrieve the release code name\nThe downside of this method is that it requires every notebook to adjust the repos and HTTPUserAgent options.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Faster Package Installs</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#cluster-environment-variable-init-script",
    "href": "chapters/pkg-management/fast-installs.html#cluster-environment-variable-init-script",
    "title": "3  Faster Package Installs",
    "section": "3.2 Cluster Environment Variable & Init Script",
    "text": "3.2 Cluster Environment Variable & Init Script\nDatabricks clusters allow specification of environment variables, there is a specific variable (DATABRICKS_DEFAULT_R_REPOS) that can be set to adjust the default repository for the entire cluster.\nYou can again refer to the wizard, the environment variables section of cluster should be:\n\nDATABRICKS_DEFAULT_R_REPOS=&lt;posit-package-manager-url-goes-here&gt;\n\nUnfortunately this isn’t as dynamic as the first option and you still need to set the HTTPUserAgent in Rprofile.site via an init script.\nThe init script will be:\n\n#!/bin/bash\n# Append changes to Rprofile.site\ncat &lt;&lt;EOF &gt;&gt; \"/etc/R/Rprofile.site\"\noptions(\n  HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version[\"platform\"], R.version[\"arch\"], R.version[\"os\"]))\n)\nEOF\n\n\n\n\n\n\n\nImportant\n\n\n\nDue to how Databricks starts up the R shell for notebook sessions it’s not straightforward to adjust the repos option in an init script alone.\nDATABRICKS_DEFAULT_R_REPOS is referenced as part of the startup process after Rprofile.site is executed and will override any earlier attempt to adjust repos.\nTherefore you’ll need to use both the init script and the environment variable configuration.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Faster Package Installs</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/fast-installs.html#setting-repo-for-cluster-library",
    "href": "chapters/pkg-management/fast-installs.html#setting-repo-for-cluster-library",
    "title": "3  Faster Package Installs",
    "section": "3.3 Setting Repo for Cluster Library",
    "text": "3.3 Setting Repo for Cluster Library\n\n\n\n\n\n\nNote\n\n\n\nSimilar to setting DATABRICKS_DEFAULT_R_REPOS this requires the HTTPUserAgent also to be set and it’s unlikely to be helpful other than for it’s purpose of installing a package to make it available for all cluster users.\n\n\nCluster libraries can install R packages and support specification of the repository.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Faster Package Installs</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html",
    "href": "chapters/pkg-management/persisting-libs.html",
    "title": "4  Persisting Packages",
    "section": "",
    "text": "4.1 Where Packages are Installed\nWhen installing packages with install.packages the default behaviour is that they’ll be installed to the first element of .libPaths().\n.libPaths() returns the paths of “R library trees”, directories that R packages can reside. When you load a package it will be loaded from the first location it is found as dictated by .libPaths().\nWhen working within a Databricks notebook .libPaths() will return 6 values by default, in order they are:\nIt’s important to understand the order defines the default behaviour as It’s possible to add or remove values in .libPaths(). You’ll almost certainly be adding values, there’s little reason to remove values.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html#where-packages-are-installed",
    "href": "chapters/pkg-management/persisting-libs.html#where-packages-are-installed",
    "title": "4  Persisting Packages",
    "section": "",
    "text": "Path\nDetails\n\n\n\n\n/local_disk0/.ephemeral_nfs/envs/rEnv-&lt;session-id&gt;\nThe first location is always a notebook specific directory, this is what allows each notebook session to have different libraries installed.\n\n\n/databricks/spark/R/lib\nOnly {SparkR} is found here\n\n\n/local_disk0/.ephemeral_nfs/cluster_libraries/r\nCluster libraries - you could also install packages here explicitly to share amongst all users (e.g. lib parameter of install.packages)\n\n\n/usr/local/lib/R/site-library\nPackages built into the Databricks Runtime\n\n\n/usr/lib/R/site-library\nEmpty\n\n\n/usr/lib/R/library\nBase R packages\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll following examples will use Unity Catalog Volumes. DBFS can be used but it’s not recommended.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html#persisting-a-package",
    "href": "chapters/pkg-management/persisting-libs.html#persisting-a-package",
    "title": "4  Persisting Packages",
    "section": "4.2 Persisting a Package",
    "text": "4.2 Persisting a Package\nThe recommended approach is to first install the library(s) you want to persist on a cluster via a notebook.\nFor example, let’s persist {leaflet} to a volume:\n\n1install.packages(\"leaflet\")\n\n# determine where the package was installed\n2pkg_location &lt;- find.package(\"leaflet\")\n\n# move package to volume\n3new_pkg_location &lt;- \"/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/my_packages\"\n4file.copy(from = pkg_location, to = new_pkg_location, recursive = TRUE)\n\n\n1\n\nInstalling {leaflet}\n\n2\n\nReturn path to package files, from what was explained before we know this will be a sub-directory of .libPaths() first path\n\n3\n\nDefine the path to volume where package will be persisted, make sure to adjust as needed\n\n4\n\nCopy the folder contents recursively to the volume\n\n\n\n\nAt this point the package is persisted, but if you restart the cluster or detach and reattach and try to load {leaflet} it will fail to load.\nThe last step is to adjust .libPaths() to include the volume path. You could make it the first value by:\n\n# adjust .libPaths\n1.libPaths(c(new_pkg_location, .libPaths()))\n\n\n1\n\nI recommend against making it the first value, will detail why in Ordering",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html#adjusting-.libpaths",
    "href": "chapters/pkg-management/persisting-libs.html#adjusting-.libpaths",
    "title": "4  Persisting Packages",
    "section": "4.3 Adjusting .libPaths()",
    "text": "4.3 Adjusting .libPaths()\n\n4.3.1 Ordering\nGiven that .libPaths() can return 6 values in a notebook you might wonder if there a “best” position to add your new volume path(s) to, that will depend on how you want packages to behave.\nA safe default is to add a path after the cluster libraries location (3rd), this will make it appear as if the Databricks Runtime has been extended to include packages in the volume path(s).\nAlternatively you could add it after the first path and all users will still have the notebook scope packages behaviour by default. It will be up to you to decide what works best.\n\n\n\n\n\n\nImportant\n\n\n\nI don’t recommend pre-pending .libPaths() with volume paths as packages will attempt to install to the first value and you cannot directly install packages to a volume path (due to volumes being backed onto cloud storage). This is why the example for persisting copies after installation.\n\n\nAn example of adjusting .libPaths() looks like:\n\nvolume_pkgs &lt;- \"/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/my_packages\"\n.libPaths(new = append(.libPaths(), volume_pkgs, after = 3))\n\n\n\n4.3.2 Helpful Functions\nThe examples can be used to build a set of functions to make this easier.\nCopying a Package\n\ncopy_package &lt;- function(name, destination) {\n  package_loc &lt;- find.package(name)\n  file.copy(from = package_loc, to = destination, recursive = TRUE)\n}\n\n# e.g. move {ggplot2} to volume\ncopy_package(\"ggplot2\", \"/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/my_packages\")\n\nAlter .libPaths()\n\nadd_lib_paths &lt;- function(path, after, version = FALSE) {\n1  if (version) {\n    rver &lt;- getRversion()\n    lib_path &lt;- file.path(path, rver)\n  } else {\n    lib_path &lt;- file.path(path)\n  }\n\n  # ensure directory exists\n  if (!file.exists(lib_path)) {\n    dir.create(lib_path, recursive = TRUE)\n  }\n\n  lib_path &lt;- normalizePath(lib_path, \"/\")\n\n  message(\"primary package path is now \", lib_path)\n  .libPaths(new = append(.libPaths(), lib_path, after = after))\n  lib_path\n}\n\n\n1\n\nAllows specifying version as TRUE or FALSE to suffix the supplied path with the current R version\n\n\n\n\n\n\n4.3.3 Avoiding Repetition\nTo avoid manually adjusting .libPaths() every notebook you can craft an init script or set environment variables, depending on the desired outcome.\n\n\n\n\n\n\nCaution\n\n\n\nIn practice this interferes with how Databricks sets up the environment, validate any changes thoroughly before rolling out to users.\n\n\n\n4.3.3.1 Init Script\n\n\n\n\n\n\nNote\n\n\n\nThis example appends to the existing Renviron.site file to ensure any settings defined as part of runtime are preserved.\nThe last two lines of the script are setting R_LIBS_SITE and R_LIBS_USER. Changing these lines can give you granular control over order for anything after the 1st value of .libPaths() as it’s injected when the notebook session starts.\n\n\n\n#!/bin/bash\n1volume_pkgs=/Volumes/&lt;catalog&gt;/&lt;schema&gt;/&lt;volume&gt;/my_packages\ncat &lt;&lt;EOF &gt;&gt; \"/etc/R/Renviron.site\"\n2R_LIBS_USER=%U:/databricks/spark/R/lib:/local_disk0/.ephemeral_nfs/cluster_libraries/r:$volume_pkgs\nEOF\n\n\n1\n\nDefine the path(s) to add to R_LIBS_USER\n\n2\n\nAppend line to /etc/R/Renviron.site with location after cluster libraries, you can rearrange the paths as long as they remain : separated\n\n\n\n\n\n\n4.3.3.2 Environment Variables\n\n\n\n\n\n\nCaution\n\n\n\nHow the Databricks Runtime defines and uses the R environment variables is something that may change and should be tested carefully, especially if upgrading runtime versions.\n\n\nThere are particular environment variables (R_LIBS, R_LIBS_USER, R_LIBS_SITE) that can be set to initialise the library search path (.libPaths()).\nR_LIBS and R_LIBS_USER are defined as part of start-up processes in Databricks Runtime and they’ll be overridden, it’s easier to adjust via an Init Script.\nR_LIBS_SITE can be set via an environment variable but is referenced by /etc/R/Renviron.site and will provides limited control over where the path will appear in the .libPaths() order (it will appear 5th, after the packages included in the Databricks runtime) unless using an init script to alter /etc/R/Renviron.site directly.",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/persisting-libs.html#organising-packages",
    "href": "chapters/pkg-management/persisting-libs.html#organising-packages",
    "title": "4  Persisting Packages",
    "section": "4.4 Organising Packages",
    "text": "4.4 Organising Packages\nWhen going down this route of persisting packages you should consider how this is organised and managed long term to avoid making things messy.\nSome practices you can consider include:\n\nMaintaining directories of packages per project, team, or user\nEnsuring directories are specific to an R version (and potentially even Databricks Runtime version)\nCoupling the use of persistence with {renv}",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Persisting Packages</span>"
    ]
  },
  {
    "objectID": "chapters/pkg-management/renv.html",
    "href": "chapters/pkg-management/renv.html",
    "title": "5  {renv}",
    "section": "",
    "text": "Under Development",
    "crumbs": [
      "Package Management",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>`{renv}`</span>"
    ]
  },
  {
    "objectID": "chapters/data-eng/odbc-dbplyr.html",
    "href": "chapters/data-eng/odbc-dbplyr.html",
    "title": "6  {dbplyr} & {odbc}",
    "section": "",
    "text": "Under Development",
    "crumbs": [
      "Data Engineering",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>`{dbplyr}` & `{odbc}`</span>"
    ]
  },
  {
    "objectID": "chapters/misc/htmlwidgets.html",
    "href": "chapters/misc/htmlwidgets.html",
    "title": "7  {htmlwidgets} in notebooks",
    "section": "",
    "text": "When you try to view a {htmlwidget} based visualisation (e.g. {leaflet}) in a Databricks notebook you’ll find there is no rendered output by default.\nThe Databricks documentation details how to get this working but requires specification of the workspace URL explicitly.\n{brickster} has a helper function that simplifies enabling {htmlwidgets}:\n\nremotes::install_github(\"zacdav-db/brickster\")\nbrickster::notebook_enable_htmlwidgets()\n\nThe function itself is straightforward, here is code (simplified version of brickster::notebook_enable_htmlwidgets) that doesn’t require installing {brickster}:\n\nenable_htmlwidgets &lt;- function(height = 450) {\n\n  # new option to control default widget height, default is 450px\n1  options(db_htmlwidget_height = height)\n\n2  system(\"apt-get --yes install pandoc\", intern = T)\n3  if (!base::require(\"htmlwidgets\")) {\n    utils::install.packages(\"htmlwidgets\")\n  }\n\n  # new method will fetch height based on new option, or default to 450px\n4  new_method &lt;- function(x, ...) {\n    x$height &lt;- getOption(\"db_htmlwidget_height\", 450)\n    file &lt;- tempfile(fileext = \".html\")\n    htmlwidgets::saveWidget(x, file = file)\n    contents &lt;- as.character(rvest::read_html(file))\n    displayHTML(contents)\n  }\n\n5  utils::assignInNamespace(\"print.htmlwidget\", new_method, ns = \"htmlwidgets\")\n  invisible(list(default_height = height, print = new_method))\n  \n}\n\n\n1\n\nThe height of the htmlwidget output is controlled via an option (db_htmlwidget_height), this allows the height to be adjusted without re-running the function\n\n2\n\nInstalling pandoc as it’s required to use htmlwidgets::saveWidget\n\n3\n\nEnsure that {htmlwidgets} is installed\n\n4\n\nFunction that writes the widget to a temporary file as a self-contained html file and then reads the contents and presents via displayHTML\n\n5\n\nOverride the htmlwidgets::print.htmlwidget method",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>`{htmlwidgets}` in notebooks</span>"
    ]
  },
  {
    "objectID": "chapters/misc/odbc-oauth.html",
    "href": "chapters/misc/odbc-oauth.html",
    "title": "8  OAuth 🤝 {odbc}",
    "section": "",
    "text": "8.1 U2M Example\nWhen running this code you should be prompted to login to the workspace or you’ll see a window that says “success”. You can close the window and continue working in R.\nlibrary(odbc)\nlibrary(DBI)\n\ncon &lt;- DBI::dbConnect(\n1  drv = odbc::databricks(),\n2  httpPath = \"/sql/1.0/warehouses/&lt;warehouse-id&gt;\",\n3  workspace = \"&lt;workspace-name&gt;.cloud.databricks.com\",\n4  authMech = 11,\n  auth_flow = 2\n)\n\n\n1\n\n{odbc} recently added odbc::databricks() to simplify connecting to Databricks (requires version &gt;=1.4.0)\n\n2\n\nThe httpPath can be found in the ‘Connection Details’ tab of a SQL warehouse\n\n3\n\nworkspace refers to the workspace URL, also found in ‘Connection Details’ tab as ‘Server hostname’\n\n4\n\nThe docs mention setting AuthMech to 11 and Auth_Flow to 2",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>OAuth 🤝 `{odbc}`</span>"
    ]
  },
  {
    "objectID": "chapters/misc/odbc-oauth.html#u2m-example",
    "href": "chapters/misc/odbc-oauth.html#u2m-example",
    "title": "8  OAuth 🤝 {odbc}",
    "section": "",
    "text": "Note\n\n\n\nOAuth U2M or OAuth 2.0 browser-based authentication works only with applications that run locally. It does not work with server-based or cloud-based applications.",
    "crumbs": [
      "Miscellaneous",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>OAuth 🤝 `{odbc}`</span>"
    ]
  }
]